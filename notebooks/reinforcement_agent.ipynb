{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Jass Agent with Reinforcement Learning\n",
    "### by Ruben Nunez & Jordan Suter\n",
    "\n",
    "This is highly inspired by the paper of Jacop Chapman and Mathias Lechner. <<Deep Q-Learning for Atari Breakout>>\n",
    "\n",
    "We tried to implement the Jass-Agent similarly as described in the paper.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<figure>\n",
    "    <img src=\"img/reinforcement.png\" width=\"600\"/>\n",
    "</figure>\n",
    "\n",
    "### table of contents:\n",
    "* Setup\n",
    "* Parameter\n",
    "* Deep Q-Network\n",
    "* Agent\n",
    "* Environment\n",
    "* Training (Jass loop)\n",
    "\n",
    "### source directory\n",
    "- https://de.mathworks.com/discovery/reinforcement-learning.html\n",
    "- https://keras.io/examples/rl/deep_q_network_breakout/\n",
    "- https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/\n",
    "- https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py\n",
    "\n",
    "UNO BOT mit sequential Model\n",
    "- https://github.com/PhilippThoelke/uno-bot/blob/master/agent.py\n",
    "- https://www.tensorflow.org/guide/keras/save_and_serialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from jass.game.game_util import *\n",
    "from jass.game.game_sim import GameSim\n",
    "from jass.game.game_observation import GameObservation\n",
    "from jass.game.const import *\n",
    "from jass.game.rule_schieber import RuleSchieber\n",
    "from jass.agents.agent import Agent\n",
    "from jass.agents.agent_random_schieber import AgentRandomSchieber\n",
    "from jass.arena.arena import Arena\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "sys.path.append(\"../deep-jass-project\")\n",
    "# noinspection PyUnresolvedReferences\n",
    "from agent_gen1 import AgentGen1\n",
    "# noinspection PyUnresolvedReferences\n",
    "from agent_gen2 import AgentGen2\n",
    "# noinspection PyUnresolvedReferences\n",
    "from agent_gen3 import AgentGen3\n",
    "# noinspection PyUnresolvedReferences\n",
    "from agent_gen4 import AgentGen4\n",
    "# noinspection PyUnresolvedReferences\n",
    "from agent_gen5 import AgentGen5\n",
    "# noinspection PyUnresolvedReferences\n",
    "from agent_intelligent import AgentIntelligent\n",
    "# noinspection PyUnresolvedReferences\n",
    "from agent_helper import get_remaining_cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "epsilon = 1.0  # Epsilon greedy parameter\n",
    "epsilon_min = 0.1  # Minimum epsilon greedy parameter\n",
    "epsilon_max = 1.0  # Maximum epsilon greedy parameter\n",
    "epsilon_interval = (epsilon_max - epsilon_min)\n",
    "batch_size = 36  # Size of batch taken from replay buffer\n",
    "max_steps_per_episode = 36 # ein Spiel geht maximal 36 ZÃ¼ge lang\n",
    "\n",
    "# In the Deepmind paper they use RMSProp however then Adam optimizer\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0) # improves training time\n",
    "model_path = 'model/model_current'\n",
    "\n",
    "# Experience replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "episode_reward_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "trick_count = 0\n",
    "\n",
    "epsilon_random_frames = 100  # Number of frames to take random action and observe output\n",
    "epsilon_greedy_frames = 1000.0  # Number of frames for exploration\n",
    "\n",
    "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
    "max_memory_length = 360 # Maximum replay length\n",
    "update_after_actions = 1  # Train the model after 1 actions\n",
    "update_target_network = 360  # How often to update the target network\n",
    "loss_function = keras.losses.Huber()  # Using huber loss for stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Deep Q-Network\n",
    "This network learns an approximation of the Q-table, which is a mapping between the states and actions that an agent will take.\n",
    "In every state are 36 actions, that can eventually be taken. The environment provides the state, and the action is chosen by selecting the larger of the 36 Q-values predicted in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-13 14:59:47.879018: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "num_actions = 36\n",
    "\n",
    "def create_UNO_model(input_size, output_size):\n",
    "\n",
    "    if os.path.exists(model_path):\n",
    "        _model = keras.models.load_model(model_path)\n",
    "        return _model\n",
    "\n",
    "    # define the model architecture\n",
    "    _model = models.Sequential()\n",
    "    _model.add(layers.Dense(units=64, activation='relu', input_shape=(input_size,)))\n",
    "    _model.add(layers.Dense(units=64, activation='relu'))\n",
    "    _model.add(layers.Dense(units=64, activation='relu'))\n",
    "    _model.add(layers.Dense(units=64, activation='relu'))\n",
    "    _model.add(layers.Dense(units=output_size, activation='linear'))\n",
    "\n",
    "    # compile the model\n",
    "    _model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "    return _model\n",
    "\n",
    "\n",
    "# The first model makes the predictions for Q-values which are used to\n",
    "# make a action.\n",
    "model = create_UNO_model(160, 36)\n",
    "# Build a target model for the prediction of future rewards.\n",
    "# The weights of a target model get updated every 10000 steps thus when the\n",
    "# loss between the Q-values is calculated the target Q-value is stable.\n",
    "model_target = create_UNO_model(160, 36)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Reinforcement Agent Implementation\n",
    "# by Ruben Nunez & Jordan Suter\n",
    "\n",
    "class AgentReinforcement(Agent):\n",
    "\n",
    "    agent = AgentIntelligent()\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._rule = RuleSchieber() # we need a rule object to determine the valid cards\n",
    "\n",
    "    def action_trump(self, obs: GameObservation) -> int:\n",
    "        return self.agent.action_trump(obs)\n",
    "\n",
    "    def action_play_card(self, obs: GameObservation, _action) -> int:\n",
    "        valid_cards = self._rule.get_valid_cards_from_obs(obs)\n",
    "        if valid_cards[_action] == 1:\n",
    "            card = _action\n",
    "        else:\n",
    "            card = np.random.choice(np.flatnonzero(valid_cards))\n",
    "\n",
    "        return card"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Environment:\n",
    "\n",
    "    game = None\n",
    "    rule = None\n",
    "\n",
    "    agent_reinforcement = None\n",
    "    agent_other = None\n",
    "\n",
    "    def __init__(self):\n",
    "        self.agent_reinforcement = AgentReinforcement()\n",
    "        self.agent_other = AgentIntelligent()\n",
    "        self.rule = RuleSchieber()\n",
    "\n",
    "    def init_agent(self):\n",
    "        self.agent_other = AgentIntelligent()\n",
    "\n",
    "    def new_game(self):\n",
    "        game = GameSim(rule=self.rule)\n",
    "\n",
    "        # deal cards\n",
    "        game.init_from_cards(hands=deal_random_hand(), dealer=SOUTH)\n",
    "        obs = game.get_observation()\n",
    "\n",
    "        trump = self.agent_reinforcement.action_trump(obs) # set trump\n",
    "        game.action_trump(trump)\n",
    "\n",
    "        if trump == PUSH: # set PUSH\n",
    "            obs = game.get_observation()\n",
    "            trump = self.agent_2.action_trump(obs) # set trump\n",
    "            game.action_trump(trump) # tell the simulation the selected trump\n",
    "\n",
    "        self.game = game\n",
    "\n",
    "    def step(self, _action):\n",
    "        obs = self.game.get_observation()\n",
    "        _reward = 0\n",
    "\n",
    "        if obs.player == 0:\n",
    "            valid_cards = self.rule.get_valid_cards_from_obs(obs)\n",
    "            card = None\n",
    "            if valid_cards[_action] == 1:\n",
    "                card = _action\n",
    "                _reward += 100\n",
    "            else:\n",
    "                card = np.random.choice(np.flatnonzero(valid_cards))\n",
    "                _reward += -100\n",
    "            self.game.action_play_card(card)\n",
    "            _reward += 5 * (obs.points[0] - obs.points[1])\n",
    "        else:\n",
    "            self.game.action_play_card(self.agent_other.action_play_card(obs))\n",
    "\n",
    "        _done = self.game.is_done()\n",
    "        _state = self.state(),\n",
    "\n",
    "        return _reward, _done\n",
    "\n",
    "\n",
    "    def state(self):\n",
    "        obs = self.game.get_observation()\n",
    "        if obs.current_trick is None:\n",
    "            obs.current_trick = np.array([-1, -1, -1, -1])\n",
    "\n",
    "        remaining_cards = get_remaining_cards(obs)\n",
    "        valid_cards = self.game.rule.get_valid_cards_from_obs(obs)\n",
    "\n",
    "        trump = np.zeros((10,))\n",
    "        trump[obs.declared_trump] = 1\n",
    "        result = np.concatenate((obs.hand, valid_cards, remaining_cards, obs.tricks.reshape(36), obs.current_trick, trump, np.array([obs.nr_cards_in_trick]),  np.array([obs.nr_played_cards])))\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training (Jass loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference: -81 @Episode: 1006347 running: reward = -1630.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mInvalidArgumentError\u001B[0m                      Traceback (most recent call last)",
      "\u001B[0;32m~/opt/anaconda3/envs/deep-jass-project/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001B[0m in \u001B[0;36mget_attr\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   2615\u001B[0m       \u001B[0;32mwith\u001B[0m \u001B[0mc_api_util\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtf_buffer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mbuf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2616\u001B[0;31m         \u001B[0mpywrap_tf_session\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTF_OperationGetAttrValueProto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_c_op\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbuf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2617\u001B[0m         \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpywrap_tf_session\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTF_GetBuffer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbuf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mInvalidArgumentError\u001B[0m: Operation 'range/limit' has no attr named '_read_only_resource_inputs'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/8m/8cy_jdw938ng23dygm9fsq3m0000gn/T/ipykernel_27917/2228032200.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     84\u001B[0m             \u001B[0;31m# Use the target model for stability\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     85\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 86\u001B[0;31m             \u001B[0mfuture_rewards\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel_target\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate_next_sample\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     87\u001B[0m             \u001B[0;31m# Q value = reward + discount factor * expected future reward\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     88\u001B[0m             updated_q_values = rewards_sample + gamma * tf.reduce_max(\n",
      "\u001B[0;32m~/opt/anaconda3/envs/deep-jass-project/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m     \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     63\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 64\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     65\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# pylint: disable=broad-except\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     66\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/deep-jass-project/lib/python3.8/site-packages/keras/engine/training.py\u001B[0m in \u001B[0;36mpredict\u001B[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1756\u001B[0m               stacklevel=2)\n\u001B[1;32m   1757\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1758\u001B[0;31m       data_handler = data_adapter.get_data_handler(\n\u001B[0m\u001B[1;32m   1759\u001B[0m           \u001B[0mx\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1760\u001B[0m           \u001B[0mbatch_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/deep-jass-project/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001B[0m in \u001B[0;36mget_data_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   1401\u001B[0m   \u001B[0;32mif\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"model\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"_cluster_coordinator\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1402\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0m_ClusterCoordinatorDataHandler\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1403\u001B[0;31m   \u001B[0;32mreturn\u001B[0m \u001B[0mDataHandler\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1404\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1405\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/deep-jass-project/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001B[0m\n\u001B[1;32m   1151\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1152\u001B[0m     \u001B[0madapter_cls\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mselect_data_adapter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1153\u001B[0;31m     self._adapter = adapter_cls(\n\u001B[0m\u001B[1;32m   1154\u001B[0m         \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1155\u001B[0m         \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/deep-jass-project/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001B[0m\n\u001B[1;32m    289\u001B[0m     \u001B[0;31m# trigger the next permutation. On the other hand, too many simultaneous\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    290\u001B[0m     \u001B[0;31m# shuffles can contend on a hardware level and degrade all performance.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 291\u001B[0;31m     \u001B[0mindices_dataset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mindices_dataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpermutation\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprefetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    292\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    293\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mslice_batch_indices\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindices\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/deep-jass-project/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001B[0m in \u001B[0;36mmap\u001B[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001B[0m\n\u001B[1;32m   2002\u001B[0m         warnings.warn(\"The `deterministic` argument has no effect unless the \"\n\u001B[1;32m   2003\u001B[0m                       \"`num_parallel_calls` argument is specified.\")\n\u001B[0;32m-> 2004\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mMapDataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmap_func\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpreserve_cardinality\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2005\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2006\u001B[0m       return ParallelMapDataset(\n",
      "\u001B[0;32m~/opt/anaconda3/envs/deep-jass-project/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001B[0m\n\u001B[1;32m   5453\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_use_inter_op_parallelism\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0muse_inter_op_parallelism\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   5454\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_preserve_cardinality\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpreserve_cardinality\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 5455\u001B[0;31m     self._map_func = StructuredFunctionWrapper(\n\u001B[0m\u001B[1;32m   5456\u001B[0m         \u001B[0mmap_func\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   5457\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_transformation_name\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/deep-jass-project/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001B[0m\n\u001B[1;32m   4531\u001B[0m         \u001B[0mfn_factory\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrace_tf_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdefun_kwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4532\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 4533\u001B[0;31m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_function\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfn_factory\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   4534\u001B[0m     \u001B[0;31m# There is no graph to add in eager mode.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4535\u001B[0m     \u001B[0madd_to_graph\u001B[0m \u001B[0;34m&=\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexecuting_eagerly\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/deep-jass-project/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36mget_concrete_function\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   3242\u001B[0m          \u001B[0;32mor\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[0;31m`\u001B[0m \u001B[0;32mor\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTensorSpec\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3243\u001B[0m     \"\"\"\n\u001B[0;32m-> 3244\u001B[0;31m     graph_function = self._get_concrete_function_garbage_collected(\n\u001B[0m\u001B[1;32m   3245\u001B[0m         *args, **kwargs)\n\u001B[1;32m   3246\u001B[0m     \u001B[0mgraph_function\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_garbage_collector\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrelease\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# pylint: disable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/deep-jass-project/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m_get_concrete_function_garbage_collected\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   3208\u001B[0m       \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3209\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3210\u001B[0;31m       \u001B[0mgraph_function\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_maybe_define_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3211\u001B[0m       \u001B[0mseen_names\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3212\u001B[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001B[0;32m~/opt/anaconda3/envs/deep-jass-project/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m_maybe_define_function\u001B[0;34m(self, args, kwargs)\u001B[0m\n\u001B[1;32m   3555\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3556\u001B[0m           \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_function_cache\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmissed\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcall_context_key\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3557\u001B[0;31m           \u001B[0mgraph_function\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_graph_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3558\u001B[0m           \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_function_cache\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprimary\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcache_key\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgraph_function\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3559\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/deep-jass-project/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m_create_graph_function\u001B[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001B[0m\n\u001B[1;32m   3390\u001B[0m     \u001B[0marg_names\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbase_arg_names\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mmissing_arg_names\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3391\u001B[0m     graph_function = ConcreteFunction(\n\u001B[0;32m-> 3392\u001B[0;31m         func_graph_module.func_graph_from_py_func(\n\u001B[0m\u001B[1;32m   3393\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_name\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3394\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_python_function\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/deep-jass-project/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001B[0m in \u001B[0;36mfunc_graph_from_py_func\u001B[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001B[0m\n\u001B[1;32m   1182\u001B[0m         if x is not None)\n\u001B[1;32m   1183\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1184\u001B[0;31m     \u001B[0mfunc_graph\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvariables\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvariables\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1185\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1186\u001B[0m   \u001B[0;32mif\u001B[0m \u001B[0madd_control_dependencies\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/deep-jass-project/lib/python3.8/site-packages/tensorflow/python/framework/auto_control_deps.py\u001B[0m in \u001B[0;36m__exit__\u001B[0;34m(self, unused_type, unused_value, unused_traceback)\u001B[0m\n\u001B[1;32m    416\u001B[0m       \u001B[0;31m# Check for any resource inputs. If we find any, we update control_inputs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    417\u001B[0m       \u001B[0;31m# and last_write_to_resource.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 418\u001B[0;31m       \u001B[0;32mfor\u001B[0m \u001B[0minp\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresource_type\u001B[0m \u001B[0;32min\u001B[0m \u001B[0m_get_resource_inputs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mop\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    419\u001B[0m         \u001B[0mis_read\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mresource_type\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mResourceType\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mREAD_ONLY\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    420\u001B[0m         \u001B[0minput_id\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtensor_id\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/deep-jass-project/lib/python3.8/site-packages/tensorflow/python/framework/auto_control_deps.py\u001B[0m in \u001B[0;36m_get_resource_inputs\u001B[0;34m(op)\u001B[0m\n\u001B[1;32m    599\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0m_get_resource_inputs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mop\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    600\u001B[0m   \u001B[0;34m\"\"\"Returns an iterable of resources touched by this `op`.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 601\u001B[0;31m   \u001B[0mreads\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwrites\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mutils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_read_write_resource_inputs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mop\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    602\u001B[0m   \u001B[0msaturated\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    603\u001B[0m   \u001B[0;32mwhile\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0msaturated\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/deep-jass-project/lib/python3.8/site-packages/tensorflow/python/framework/auto_control_deps_utils.py\u001B[0m in \u001B[0;36mget_read_write_resource_inputs\u001B[0;34m(op)\u001B[0m\n\u001B[1;32m    107\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    108\u001B[0m   \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 109\u001B[0;31m     \u001B[0mread_only_input_indices\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mop\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_attr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mREAD_ONLY_RESOURCE_INPUTS_ATTR\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    110\u001B[0m   \u001B[0;32mexcept\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    111\u001B[0m     \u001B[0;31m# Attr was not set. Add all resource inputs to `writes` and return.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/deep-jass-project/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001B[0m in \u001B[0;36mget_attr\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   2616\u001B[0m         \u001B[0mpywrap_tf_session\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTF_OperationGetAttrValueProto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_c_op\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbuf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2617\u001B[0m         \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpywrap_tf_session\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTF_GetBuffer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbuf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2618\u001B[0;31m     \u001B[0;32mexcept\u001B[0m \u001B[0merrors\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mInvalidArgumentError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2619\u001B[0m       \u001B[0;31m# Convert to ValueError for backwards compatibility.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2620\u001B[0m       \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmessage\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "env = Environment()\n",
    "\n",
    "# In the Deepmind paper they use RMSProp however then Adam optimizer\n",
    "# improves training time\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "\n",
    "# Experience replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "episode_reward_history = []\n",
    "running_reward = 0\n",
    "episode_count = 1000000\n",
    "move_count = 0\n",
    "\n",
    "# Maximum replay length\n",
    "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
    "max_memory_length = 100000\n",
    "update_after_actions = 4 # Train the model after 4 actions\n",
    "update_target_network = 1000 # How often to update the target network\n",
    "loss_function = keras.losses.Huber() # Using huber loss for stability\n",
    "\n",
    "\n",
    "while True:  # Run until solved\n",
    "    env.new_game() # Start Game (80,)\n",
    "    state = env.state()\n",
    "    episode_reward = 0\n",
    "\n",
    "    for step in range(0, max_steps_per_episode):\n",
    "\n",
    "        # of the agent in a pop up window.\n",
    "        move_count += 1\n",
    "\n",
    "        # Use epsilon-greedy for exploration\n",
    "        if move_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            # Take random action\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # Predict action Q-values\n",
    "            # From environment state\n",
    "            state_tensor = tf.convert_to_tensor(state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            action_probs = model(state_tensor, training=False)\n",
    "            # Take best action\n",
    "            action = tf.argmax(action_probs[0]).numpy()\n",
    "\n",
    "        # Decay probability of taking random action\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "        # Apply the sampled action in our environment\n",
    "        reward, done = env.step(action)\n",
    "        state_next = env.state()\n",
    "        # state_next = np.array(state_next)\n",
    "\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Save actions and states in replay buffer\n",
    "        action_history.append(action)\n",
    "        state_history.append(state)\n",
    "        state_next_history.append(state_next)\n",
    "        done_history.append(done)\n",
    "        rewards_history.append(reward)\n",
    "        state = state_next\n",
    "\n",
    "        # Update every fourth frame and once batch size is over 32\n",
    "        if move_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "\n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
    "\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = np.array([state_history[i] for i in indices])\n",
    "            state_next_sample = np.array([state_next_history[i] for i in indices])\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            done_sample = tf.convert_to_tensor(\n",
    "                [float(done_history[i]) for i in indices]\n",
    "            )\n",
    "\n",
    "            # Build the updated Q-values for the sampled future states\n",
    "            # Use the target model for stability\n",
    "\n",
    "            future_rewards = model_target.predict(state_next_sample)\n",
    "            # Q value = reward + discount factor * expected future reward\n",
    "            updated_q_values = rewards_sample + gamma * tf.reduce_max(\n",
    "                future_rewards, axis=1\n",
    "            )\n",
    "\n",
    "            # If final frame set the last value to -1\n",
    "            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
    "\n",
    "            # Create a mask so we only calculate loss on the updated Q-values\n",
    "            masks = tf.one_hot(action_sample, num_actions)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Train the model on the states and updated Q-values\n",
    "                q_values = model(state_sample)\n",
    "\n",
    "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                # Calculate loss between new Q-value and old Q-value\n",
    "                loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "            # Backpropagation\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if move_count % update_target_network == 0:\n",
    "            # update the the target network with new weights\n",
    "            model_target.set_weights(model.get_weights())\n",
    "            # Log details\n",
    "            template = \"running reward: {:.2f} at episode {}, frame count {}\"\n",
    "            print(template.format(running_reward, episode_count, move_count))\n",
    "\n",
    "        # Limit the state and reward history\n",
    "        if len(rewards_history) > max_memory_length:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "\n",
    "        if done:\n",
    "            a = 0\n",
    "            break\n",
    "\n",
    "    # Update running reward to check condition for solving\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    if len(episode_reward_history) > 100:\n",
    "        del episode_reward_history[:1]\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "\n",
    "    episode_count += 1\n",
    "\n",
    "    if episode_count % 100 == 0:\n",
    "        model.save(model_path)\n",
    "        env.init_agent()\n",
    "\n",
    "    if episode_count % 10000 == 0:\n",
    "        model.save('model/model_' + str(episode_count))\n",
    "\n",
    "    from IPython.display import clear_output, display\n",
    "    clear_output(wait=True)\n",
    "    print(\"difference: \" + str(env.game.get_observation().points[0] - env.game.get_observation().points[1]) + \" @Episode: \" + str(episode_count) + \" running: reward = \"  + str(running_reward), flush=True)\n",
    "\n",
    "    if running_reward > 40000:  # Condition to consider the task solved\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}