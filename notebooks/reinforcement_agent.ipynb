{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Jass Agent with Reinforcement Learning\n",
    "### Fact sheet by Ruben Nunez & Jordan Suter\n",
    "\n",
    "This fact sheet is highly inspired by the paper of Jacop Chapman and Mathias Lechner. <<Deep Q-Learning for Atari Breakout>>\n",
    "\n",
    "We tried to implement the Jass-Agent similarly as described in the paper.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<figure>\n",
    "    <img src=\"img/reinforcement.png\" width=\"600\"/>\n",
    "</figure>\n",
    "\n",
    "### table of contents:\n",
    "* Setup\n",
    "* Parameter\n",
    "* Deep Q-Network\n",
    "* Agent\n",
    "* Environment\n",
    "* Training (Jass loop)\n",
    "\n",
    "### source directory\n",
    "- https://de.mathworks.com/discovery/reinforcement-learning.html\n",
    "- https://keras.io/examples/rl/deep_q_network_breakout/\n",
    "- https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/\n",
    "- https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py\n",
    "\n",
    "UNO BOT mit sequential Model\n",
    "- https://github.com/PhilippThoelke/uno-bot/blob/master/agent.py\n",
    "- https://www.tensorflow.org/guide/keras/save_and_serialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from jass.game.game_util import *\n",
    "from jass.game.game_sim import GameSim\n",
    "from jass.game.game_observation import GameObservation\n",
    "from jass.game.const import *\n",
    "from jass.game.rule_schieber import RuleSchieber\n",
    "from jass.agents.agent import Agent\n",
    "from jass.agents.agent_random_schieber import AgentRandomSchieber\n",
    "from jass.arena.arena import Arena\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "sys.path.append(\"../deep-jass-project\")\n",
    "# noinspection PyUnresolvedReferences\n",
    "from agent_gen1 import AgentGen1\n",
    "# noinspection PyUnresolvedReferences\n",
    "from agent_gen2 import AgentGen2\n",
    "# noinspection PyUnresolvedReferences\n",
    "from agent_gen3 import AgentGen3\n",
    "# noinspection PyUnresolvedReferences\n",
    "from agent_gen4 import AgentGen4\n",
    "# noinspection PyUnresolvedReferences\n",
    "from agent_gen5 import AgentGen5\n",
    "# noinspection PyUnresolvedReferences\n",
    "from agent_intelligent import AgentIntelligent\n",
    "# noinspection PyUnresolvedReferences\n",
    "from agent_helper import get_remaining_cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "epsilon = 1.0  # Epsilon greedy parameter\n",
    "epsilon_min = 0.1  # Minimum epsilon greedy parameter\n",
    "epsilon_max = 1.0  # Maximum epsilon greedy parameter\n",
    "epsilon_interval = (epsilon_max - epsilon_min)\n",
    "batch_size = 32  # Size of batch taken from replay buffer\n",
    "max_steps_per_episode = 36 # ein Spiel geht maximal 36 ZÃ¼ge lang\n",
    "\n",
    "# In the Deepmind paper they use RMSProp however then Adam optimizer\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0) # improves training time\n",
    "model_path = 'model/model_current'\n",
    "\n",
    "# Experience replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "episode_reward_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "trick_count = 0\n",
    "\n",
    "epsilon_random_frames = 1000  # Number of frames to take random action and observe output\n",
    "epsilon_greedy_frames = 1000000.0  # Number of frames for exploration\n",
    "\n",
    "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
    "max_memory_length = 100000 # Maximum replay length\n",
    "update_after_actions = 1  # Train the model after 1 actions\n",
    "update_target_network = 10000  # How often to update the target network\n",
    "loss_function = keras.losses.Huber()  # Using huber loss for stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Deep Q-Network\n",
    "This network learns an approximation of the Q-table, which is a mapping between the states and actions that an agent will take.\n",
    "In every state are 36 actions, that can eventually be taken. The environment provides the state, and the action is chosen by selecting the larger of the 36 Q-values predicted in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_actions = 36\n",
    "\n",
    "def create_UNO_model(input_size, output_size):\n",
    "\n",
    "    if os.path.exists(model_path):\n",
    "        _model = keras.models.load_model(model_path)\n",
    "        return _model\n",
    "\n",
    "    # define the model architecture\n",
    "    _model = models.Sequential()\n",
    "    _model.add(layers.Dense(units=64, activation='relu', input_shape=(input_size,)))\n",
    "    _model.add(layers.Dense(units=64, activation='relu'))\n",
    "    _model.add(layers.Dense(units=64, activation='relu'))\n",
    "    _model.add(layers.Dense(units=output_size, activation='linear'))\n",
    "\n",
    "    # compile the model\n",
    "    _model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "    return _model\n",
    "\n",
    "\n",
    "# The first model makes the predictions for Q-values which are used to\n",
    "# make a action.\n",
    "model = create_UNO_model(151, 36)\n",
    "# Build a target model for the prediction of future rewards.\n",
    "# The weights of a target model get updated every 10000 steps thus when the\n",
    "# loss between the Q-values is calculated the target Q-value is stable.\n",
    "model_target = create_UNO_model(151, 36)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Reinforcement Agent Implementation\n",
    "# by Ruben Nunez & Jordan Suter\n",
    "\n",
    "class AgentReinforcement(Agent):\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_trump_selection_score(cards, trump: int) -> int:\n",
    "        trump_score = [15, 10, 7, 25, 6, 19, 5, 5, 5]\n",
    "        no_trump_score = [9, 7, 5, 2, 1, 0, 0, 0, 0]\n",
    "\n",
    "        score = 0\n",
    "        for card in cards:\n",
    "            suit = int(card / 9)\n",
    "            exact_card = card % 9\n",
    "            score += trump_score[exact_card] if trump == suit else no_trump_score[exact_card]\n",
    "\n",
    "        return score\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._rule = RuleSchieber() # we need a rule object to determine the valid cards\n",
    "\n",
    "    def action_trump(self, obs: GameObservation) -> int:\n",
    "        card_list = convert_one_hot_encoded_cards_to_int_encoded_list(obs.hand)\n",
    "        threshold = 68\n",
    "        scores = [0, 0, 0, 0]\n",
    "        for suit in range(0, 4):\n",
    "            trump_score = self.calculate_trump_selection_score(card_list, suit)\n",
    "            scores[suit] = trump_score\n",
    "\n",
    "        best_score = max(scores)\n",
    "        best_suit = scores.index(best_score)\n",
    "\n",
    "        if best_score <= threshold and obs.player < 1:\n",
    "            return PUSH\n",
    "        else:\n",
    "            return best_suit\n",
    "\n",
    "    def action_play_card(self, obs: GameObservation, _action) -> int:\n",
    "        valid_cards = self._rule.get_valid_cards_from_obs(obs)\n",
    "        if valid_cards[_action] == 1:\n",
    "            card = _action\n",
    "        else:\n",
    "            card = np.random.choice(np.flatnonzero(valid_cards))\n",
    "\n",
    "        return card"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Environment:\n",
    "\n",
    "    game = None\n",
    "    rule = None\n",
    "\n",
    "    agent_reinforcement = None\n",
    "    agent_other = None\n",
    "\n",
    "    def __init__(self):\n",
    "        self.agent_reinforcement = AgentReinforcement()\n",
    "        self.agent_other = AgentIntelligent()\n",
    "        self.rule = RuleSchieber()\n",
    "\n",
    "    def init_agent(self):\n",
    "        self.agent_other = AgentIntelligent()\n",
    "\n",
    "    def new_game(self):\n",
    "        game = GameSim(rule=self.rule)\n",
    "\n",
    "        # deal cards\n",
    "        game.init_from_cards(hands=deal_random_hand(), dealer=SOUTH)\n",
    "        obs = game.get_observation()\n",
    "\n",
    "        trump = self.agent_reinforcement.action_trump(obs) # set trump\n",
    "        game.action_trump(trump)\n",
    "\n",
    "        if trump == PUSH: # set PUSH\n",
    "            obs = game.get_observation()\n",
    "            trump = self.agent_2.action_trump(obs) # set trump\n",
    "            game.action_trump(trump) # tell the simulation the selected trump\n",
    "\n",
    "        self.game = game\n",
    "\n",
    "    def step(self, _action):\n",
    "        obs = self.game.get_observation()\n",
    "        _reward = 0\n",
    "\n",
    "        if obs.player == 0:\n",
    "            valid_cards = self.rule.get_valid_cards_from_obs(obs)\n",
    "            card = None\n",
    "            if valid_cards[_action] == 1:\n",
    "                card = _action\n",
    "            else:\n",
    "                card = np.random.choice(np.flatnonzero(valid_cards))\n",
    "                _reward += -100\n",
    "            self.game.action_play_card(card)\n",
    "            _reward += obs.points[0] - obs.points[1]\n",
    "        else:\n",
    "            self.game.action_play_card(self.agent_other.action_play_card(obs))\n",
    "\n",
    "        _done = self.game.is_done()\n",
    "        _state = self.state(),\n",
    "\n",
    "        return _reward, _done\n",
    "\n",
    "\n",
    "    def state(self):\n",
    "        obs = self.game.get_observation()\n",
    "        if obs.current_trick is None:\n",
    "            obs.current_trick = np.array([-1, -1, -1, -1])\n",
    "\n",
    "        remaining_cards = get_remaining_cards(obs)\n",
    "        valid_cards = self.game.rule.get_valid_cards_from_obs(obs)\n",
    "\n",
    "        result = np.concatenate((obs.hand, valid_cards, remaining_cards, obs.tricks.reshape(36), obs.current_trick, np.array([obs.declared_trump]), np.array([obs.nr_cards_in_trick]),  np.array([obs.nr_played_cards])))\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training (Jass loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference: -81 @Episode: 23126 running: reward = -942.34\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14984/1947289254.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[1;31m# Backpropagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m             \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mK:\\ProgramData\\Anaconda\\envs\\deep-jass-project\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1082\u001b[0m                           for x in nest.flatten(output_gradients)]\n\u001b[0;32m   1083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1084\u001b[1;33m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[0;32m   1085\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1086\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mK:\\ProgramData\\Anaconda\\envs\\deep-jass-project\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[0;32m     72\u001b[0m       \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mK:\\ProgramData\\Anaconda\\envs\\deep-jass-project\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    157\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mK:\\ProgramData\\Anaconda\\envs\\deep-jass-project\\lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py\u001b[0m in \u001b[0;36m_BiasAddGrad\u001b[1;34m(op, received_grad)\u001b[0m\n\u001b[0;32m    345\u001b[0m     \u001b[0mdata_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m   return (received_grad,\n\u001b[1;32m--> 347\u001b[1;33m           gen_nn_ops.bias_add_grad(\n\u001b[0m\u001b[0;32m    348\u001b[0m               out_backprop=received_grad, data_format=data_format))\n\u001b[0;32m    349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mK:\\ProgramData\\Anaconda\\envs\\deep-jass-project\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mbias_add_grad\u001b[1;34m(out_backprop, data_format, name)\u001b[0m\n\u001b[0;32m    746\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 748\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m    749\u001b[0m         _ctx, \"BiasAddGrad\", name, out_backprop, \"data_format\", data_format)\n\u001b[0;32m    750\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = Environment()\n",
    "\n",
    "# In the Deepmind paper they use RMSProp however then Adam optimizer\n",
    "# improves training time\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "\n",
    "# Experience replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "episode_reward_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "move_count = 0\n",
    "\n",
    "# Number of frames for exploration\n",
    "epsilon_greedy_frames = 1000000.0\n",
    "# Maximum replay length\n",
    "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
    "max_memory_length = 100000\n",
    "update_after_actions = 4 # Train the model after 4 actions\n",
    "update_target_network = 1000 # How often to update the target network\n",
    "loss_function = keras.losses.Huber() # Using huber loss for stability\n",
    "\n",
    "\n",
    "while True:  # Run until solved\n",
    "    env.new_game() # Start Game (80,)\n",
    "    state = env.state()\n",
    "    episode_reward = 0\n",
    "\n",
    "    for step in range(0, max_steps_per_episode):\n",
    "\n",
    "        # of the agent in a pop up window.\n",
    "        move_count += 1\n",
    "\n",
    "        # Use epsilon-greedy for exploration\n",
    "        if move_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            # Take random action\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # Predict action Q-values\n",
    "            # From environment state\n",
    "            state_tensor = tf.convert_to_tensor(state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            action_probs = model(state_tensor, training=False)\n",
    "            # Take best action\n",
    "            action = tf.argmax(action_probs[0]).numpy()\n",
    "\n",
    "        # Decay probability of taking random action\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "        # Apply the sampled action in our environment\n",
    "        reward, done = env.step(action)\n",
    "        state_next = env.state()\n",
    "        # state_next = np.array(state_next)\n",
    "\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Save actions and states in replay buffer\n",
    "        action_history.append(action)\n",
    "        state_history.append(state)\n",
    "        state_next_history.append(state_next)\n",
    "        done_history.append(done)\n",
    "        rewards_history.append(reward)\n",
    "        state = state_next\n",
    "\n",
    "        # Update every fourth frame and once batch size is over 32\n",
    "        if move_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "\n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
    "\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = np.array([state_history[i] for i in indices])\n",
    "            state_next_sample = np.array([state_next_history[i] for i in indices])\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            done_sample = tf.convert_to_tensor(\n",
    "                [float(done_history[i]) for i in indices]\n",
    "            )\n",
    "\n",
    "            # Build the updated Q-values for the sampled future states\n",
    "            # Use the target model for stability\n",
    "\n",
    "            future_rewards = model_target.predict(state_next_sample)\n",
    "            # Q value = reward + discount factor * expected future reward\n",
    "            updated_q_values = rewards_sample + gamma * tf.reduce_max(\n",
    "                future_rewards, axis=1\n",
    "            )\n",
    "\n",
    "            # If final frame set the last value to -1\n",
    "            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
    "\n",
    "            # Create a mask so we only calculate loss on the updated Q-values\n",
    "            masks = tf.one_hot(action_sample, num_actions)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Train the model on the states and updated Q-values\n",
    "                q_values = model(state_sample)\n",
    "\n",
    "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                # Calculate loss between new Q-value and old Q-value\n",
    "                loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "            # Backpropagation\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if move_count % update_target_network == 0:\n",
    "            # update the the target network with new weights\n",
    "            model_target.set_weights(model.get_weights())\n",
    "            # Log details\n",
    "            template = \"running reward: {:.2f} at episode {}, frame count {}\"\n",
    "            print(template.format(running_reward, episode_count, move_count))\n",
    "\n",
    "        # Limit the state and reward history\n",
    "        if len(rewards_history) > max_memory_length:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "\n",
    "        if done:\n",
    "            a = 0\n",
    "            break\n",
    "\n",
    "    # Update running reward to check condition for solving\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    if len(episode_reward_history) > 100:\n",
    "        del episode_reward_history[:1]\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "\n",
    "    episode_count += 1\n",
    "\n",
    "    if episode_count % 100 == 0:\n",
    "        model.save(model_path)\n",
    "        env.init_agent()\n",
    "\n",
    "    if episode_count % 1000 == 0:\n",
    "        model.save('model/model_' + str(episode_count))\n",
    "\n",
    "    from IPython.display import clear_output, display\n",
    "    clear_output(wait=True)\n",
    "    print(\"difference: \" + str(env.game.get_observation().points[0] - env.game.get_observation().points[1]) + \" @Episode: \" + str(episode_count) + \" running: reward = \"  + str(running_reward), flush=True)\n",
    "\n",
    "    if running_reward > 40000:  # Condition to consider the task solved\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
