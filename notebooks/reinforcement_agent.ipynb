{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Jass Agent with Reinforcement Learning\n",
    "### Fact sheet by Ruben Nunez & Jordan Suter\n",
    "\n",
    "This fact sheet is highly inspired by the paper of Jacop Chapman and Mathias Lechner. <<Deep Q-Learning for Atari Breakout>>\n",
    "\n",
    "We tried to implement the Jass-Agent similarly as described in the paper.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<figure>\n",
    "    <img src=\"img/reinforcement.png\" width=\"600\"/>\n",
    "</figure>\n",
    "\n",
    "### table of contents:\n",
    "* Setup\n",
    "* Parameter\n",
    "* Deep Q-Network\n",
    "* Agent\n",
    "* Environment\n",
    "* Training (Jass loop)\n",
    "\n",
    "### source directory\n",
    "- https://de.mathworks.com/discovery/reinforcement-learning.html\n",
    "- https://keras.io/examples/rl/deep_q_network_breakout/\n",
    "- https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/\n",
    "\n",
    "UNO BOT mit sequential Model\n",
    "- https://github.com/PhilippThoelke/uno-bot/blob/master/agent.py\n",
    "- https://www.tensorflow.org/guide/keras/save_and_serialize"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from jass.game.game_util import *\n",
    "from jass.game.game_sim import GameSim\n",
    "from jass.game.game_observation import GameObservation\n",
    "from jass.game.const import *\n",
    "from jass.game.rule_schieber import RuleSchieber\n",
    "from jass.agents.agent import Agent\n",
    "from jass.agents.agent_random_schieber import AgentRandomSchieber\n",
    "from jass.arena.arena import Arena\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "sys.path.append(\"../deep-jass-project\")\n",
    "# noinspection PyUnresolvedReferences\n",
    "from agent_gen1 import AgentGen1\n",
    "# noinspection PyUnresolvedReferences\n",
    "from agent_gen2 import AgentGen2\n",
    "# noinspection PyUnresolvedReferences\n",
    "from agent_gen3 import AgentGen3\n",
    "# noinspection PyUnresolvedReferences\n",
    "from agent_gen4 import AgentGen4\n",
    "# noinspection PyUnresolvedReferences\n",
    "from agent_gen5 import AgentGen5\n",
    "# noinspection PyUnresolvedReferences\n",
    "from agent_helper import get_remaining_cards"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parameter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "seed = 42\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "epsilon = 1.0  # Epsilon greedy parameter\n",
    "epsilon_min = 0.1  # Minimum epsilon greedy parameter\n",
    "epsilon_max = 1.0  # Maximum epsilon greedy parameter\n",
    "epsilon_interval = (epsilon_max - epsilon_min)\n",
    "batch_size = 32  # Size of batch taken from replay buffer\n",
    "max_steps_per_episode = 10000\n",
    "\n",
    "# In the Deepmind paper they use RMSProp however then Adam optimizer\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0) # improves training time\n",
    "\n",
    "# Experience replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "episode_reward_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "trick_count = 0\n",
    "\n",
    "epsilon_random_frames = 50000  # Number of frames to take random action and observe output\n",
    "epsilon_greedy_frames = 1000000.0  # Number of frames for exploration\n",
    "\n",
    "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
    "max_memory_length = 100000 # Maximum replay length\n",
    "update_after_actions = 1  # Train the model after 1 actions\n",
    "update_target_network = 10000  # How often to update the target network\n",
    "loss_function = keras.losses.Huber()  # Using huber loss for stability"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deep Q-Network\n",
    "This network learns an approximation of the Q-table, which is a mapping between the states and actions that an agent will take.\n",
    "In every state are 36 actions, that can eventually be taken. The environment provides the state, and the action is chosen by selecting the larger of the 36 Q-values predicted in the output layer."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "num_actions = 36\n",
    "\n",
    "def create_UNO_model(input_size, output_size):\n",
    "    # define the model architecture\n",
    "    _model = models.Sequential()\n",
    "    _model.add(layers.Dense(units=64, activation='relu', input_shape=(input_size,)))\n",
    "    _model.add(layers.Dense(units=64, activation='relu'))\n",
    "    _model.add(layers.Dense(units=64, activation='relu'))\n",
    "    _model.add(layers.Dense(units=output_size, activation='linear'))\n",
    "\n",
    "    # compile the model\n",
    "    _model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "    return _model\n",
    "\n",
    "\n",
    "def create_q_model():\n",
    "    # inputs = layers.Input(shape=(84, 84, 4,)) # Network defined by the Deepmind paper [0,0,0,0,0,0,...,0,0,0,0,.......,0,0,0,..1,0,0,0,0,0]\n",
    "    inputs = layers.Input(shape=(80,)) # Network defined by the Deepmind paper [0,0,0,0,0,0,...,0,0,0,0,.......,0,0,0,..1,0,0,0,0,0]\n",
    "\n",
    "    # Convolutions on the frames on the screen\n",
    "    #layer1 = layers.Conv2D(16, 2, strides=4, activation=\"relu\")(inputs)\n",
    "    layer1 = layers.Conv1D(16, 1, strides=4, activation=\"relu\")(inputs)\n",
    "    #layer2 = layers.Conv2D(64, 1, strides=2, activation=\"relu\")(layer1)\n",
    "    layer2 = layers.Conv1D(16, strides=4, activation=\"relu\")(layer1)\n",
    "    #layer3 = layers.Conv2D(64, 1, strides=1, activation=\"relu\")(layer2)\n",
    "    layer3 = layers.Conv1D(16, strides=4, activation=\"relu\")(layer2)\n",
    "\n",
    "    layer4 = layers.Flatten()(layer3)\n",
    "\n",
    "    layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n",
    "    action = layers.Dense(num_actions, activation=\"linear\")(layer5)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=action)\n",
    "\n",
    "# The first model makes the predictions for Q-values which are used to\n",
    "# make a action.\n",
    "model = create_UNO_model(151, 36)\n",
    "# Build a target model for the prediction of future rewards.\n",
    "# The weights of a target model get updated every 10000 steps thus when the\n",
    "# loss between the Q-values is calculated the target Q-value is stable.\n",
    "model_target = create_UNO_model(151, 36)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Agent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "# Reinforcement Agent Implementation\n",
    "# by Ruben Nunez & Jordan Suter\n",
    "\n",
    "class AgentReinforcement(Agent):\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_trump_selection_score(cards, trump: int) -> int:\n",
    "        trump_score = [15, 10, 7, 25, 6, 19, 5, 5, 5]\n",
    "        no_trump_score = [9, 7, 5, 2, 1, 0, 0, 0, 0]\n",
    "\n",
    "        score = 0\n",
    "        for card in cards:\n",
    "            suit = int(card / 9)\n",
    "            exact_card = card % 9\n",
    "            score += trump_score[exact_card] if trump == suit else no_trump_score[exact_card]\n",
    "\n",
    "        return score\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._rule = RuleSchieber() # we need a rule object to determine the valid cards\n",
    "\n",
    "    def action_trump(self, obs: GameObservation) -> int:\n",
    "        card_list = convert_one_hot_encoded_cards_to_int_encoded_list(obs.hand)\n",
    "        threshold = 68\n",
    "        scores = [0, 0, 0, 0]\n",
    "        for suit in range(0, 4):\n",
    "            trump_score = self.calculate_trump_selection_score(card_list, suit)\n",
    "            scores[suit] = trump_score\n",
    "\n",
    "        best_score = max(scores)\n",
    "        best_suit = scores.index(best_score)\n",
    "\n",
    "        if best_score <= threshold and obs.player < 1:\n",
    "            return PUSH\n",
    "        else:\n",
    "            return best_suit\n",
    "\n",
    "    def action_play_card(self, obs: GameObservation, _action) -> int:\n",
    "        valid_cards = self._rule.get_valid_cards_from_obs(obs)\n",
    "        if valid_cards[_action] == 1:\n",
    "            card = _action\n",
    "        else:\n",
    "            card = np.random.choice(np.flatnonzero(valid_cards))\n",
    "\n",
    "        return card"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Environment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "class Environment:\n",
    "\n",
    "    game = None\n",
    "\n",
    "    agent_reinforcement = None\n",
    "    agent_1 = None\n",
    "    agent_2 = None\n",
    "    agent_3 = None\n",
    "\n",
    "    def __init__(self):\n",
    "        self.agent_reinforcement = AgentReinforcement()\n",
    "        self.agent_1 = AgentGen1()\n",
    "        self.agent_2 = AgentGen1()\n",
    "        self.agent_3 = AgentGen1()\n",
    "\n",
    "    def new_game(self):\n",
    "        rule = RuleSchieber()\n",
    "        game = GameSim(rule=rule)\n",
    "\n",
    "        # deal cards\n",
    "        game.init_from_cards(hands=deal_random_hand(), dealer=SOUTH)\n",
    "        obs = game.get_observation()\n",
    "\n",
    "        trump = self.agent_reinforcement.action_trump(obs) # set trump\n",
    "        game.action_trump(trump)\n",
    "\n",
    "        if trump == PUSH: # set PUSH\n",
    "            obs = game.get_observation()\n",
    "            trump = self.agent_2.action_trump(obs) # set trump\n",
    "            game.action_trump(trump) # tell the simulation the selected trump\n",
    "\n",
    "        self.game = game\n",
    "\n",
    "    def step(self, _action):\n",
    "        obs = self.game.get_observation()\n",
    "\n",
    "        if obs.player == 0:\n",
    "            self.game.action_play_card(self.agent_reinforcement.action_play_card(obs, _action))\n",
    "        if obs.player == 1:\n",
    "            self.game.action_play_card(self.agent_1.action_play_card(obs))\n",
    "        if obs.player == 2:\n",
    "            self.game.action_play_card(self.agent_2.action_play_card(obs))\n",
    "        if obs.player == 3:\n",
    "            self.game.action_play_card(self.agent_3.action_play_card(obs))\n",
    "\n",
    "        _done = self.game.is_done()\n",
    "        _state = self.state(),\n",
    "        _reward = 0\n",
    "\n",
    "        if obs.player == 0:\n",
    "            _reward = obs.points[0] - obs.points[1]\n",
    "\n",
    "        return _reward, _done\n",
    "\n",
    "\n",
    "    def state(self):\n",
    "        obs = self.game.get_observation()\n",
    "        if obs.current_trick is None:\n",
    "            obs.current_trick = np.array([-1, -1, -1, -1])\n",
    "\n",
    "        remaining_cards = get_remaining_cards(obs)\n",
    "        valid_cards = self.game.rule.get_valid_cards_from_obs(obs)\n",
    "\n",
    "        result = np.concatenate((obs.hand, valid_cards, remaining_cards, obs.tricks.reshape(36), obs.current_trick, np.array([obs.declared_trump]), np.array([obs.nr_cards_in_trick]),  np.array([obs.nr_played_cards])))\n",
    "\n",
    "        return result\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training (Jass loop)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference: -79 @Episode: 1 running: reward = -44.0\n",
      "difference: -41 @Episode: 2 running: reward = -90.5\n",
      "difference: -135 @Episode: 3 running: reward = -230.66666666666666\n",
      "difference: -87 @Episode: 4 running: reward = -267.25\n",
      "difference: -77 @Episode: 5 running: reward = -291.6\n",
      "difference: -1 @Episode: 6 running: reward = -216.5\n",
      "difference: 37 @Episode: 7 running: reward = -199.42857142857142\n",
      "difference: 17 @Episode: 8 running: reward = -221.75\n",
      "difference: -77 @Episode: 9 running: reward = -226.77777777777777\n",
      "difference: 17 @Episode: 10 running: reward = -221.6\n",
      "difference: -25 @Episode: 11 running: reward = -214.72727272727272\n",
      "difference: 55 @Episode: 12 running: reward = -191.08333333333334\n",
      "difference: -129 @Episode: 13 running: reward = -215.07692307692307\n",
      "difference: -103 @Episode: 14 running: reward = -229.42857142857142\n",
      "difference: -95 @Episode: 15 running: reward = -222.33333333333334\n",
      "difference: -11 @Episode: 16 running: reward = -205.5625\n",
      "difference: -21 @Episode: 17 running: reward = -186.0\n",
      "difference: 33 @Episode: 18 running: reward = -163.5\n",
      "difference: 55 @Episode: 19 running: reward = -131.1578947368421\n",
      "difference: -95 @Episode: 20 running: reward = -143.1\n",
      "difference: -51 @Episode: 21 running: reward = -156.1904761904762\n",
      "difference: -41 @Episode: 22 running: reward = -152.5909090909091\n",
      "difference: -51 @Episode: 23 running: reward = -153.3913043478261\n",
      "difference: -31 @Episode: 24 running: reward = -158.5\n",
      "difference: -45 @Episode: 25 running: reward = -166.04\n",
      "difference: 101 @Episode: 26 running: reward = -149.19230769230768\n",
      "difference: 57 @Episode: 27 running: reward = -143.62962962962962\n",
      "difference: 47 @Episode: 28 running: reward = -121.35714285714286\n",
      "difference: 9 @Episode: 29 running: reward = -123.03448275862068\n",
      "difference: 73 @Episode: 30 running: reward = -115.9\n",
      "difference: -125 @Episode: 31 running: reward = -131.61290322580646\n",
      "difference: -13 @Episode: 32 running: reward = -130.15625\n",
      "difference: -19 @Episode: 33 running: reward = -129.8181818181818\n",
      "difference: -17 @Episode: 34 running: reward = -129.6764705882353\n",
      "difference: -33 @Episode: 35 running: reward = -134.05714285714285\n",
      "difference: -47 @Episode: 36 running: reward = -135.75\n",
      "difference: 9 @Episode: 37 running: reward = -124.16216216216216\n",
      "difference: 17 @Episode: 38 running: reward = -127.26315789473684\n",
      "difference: -81 @Episode: 39 running: reward = -129.43589743589743\n",
      "difference: -109 @Episode: 40 running: reward = -140.975\n",
      "difference: 11 @Episode: 41 running: reward = -142.0487804878049\n",
      "difference: -29 @Episode: 42 running: reward = -145.73809523809524\n",
      "difference: -29 @Episode: 43 running: reward = -145.58139534883722\n",
      "difference: 95 @Episode: 44 running: reward = -129.36363636363637\n",
      "difference: -63 @Episode: 45 running: reward = -128.77777777777777\n",
      "difference: -73 @Episode: 46 running: reward = -130.41304347826087\n",
      "difference: -61 @Episode: 47 running: reward = -130.9787234042553\n",
      "difference: -19 @Episode: 48 running: reward = -129.08333333333334\n",
      "difference: -75 @Episode: 49 running: reward = -135.59183673469389\n",
      "difference: -9 @Episode: 50 running: reward = -133.26\n",
      "difference: -119 @Episode: 51 running: reward = -141.84313725490196\n",
      "difference: -107 @Episode: 52 running: reward = -148.73076923076923\n",
      "difference: -1 @Episode: 53 running: reward = -146.03773584905662\n",
      "difference: -71 @Episode: 54 running: reward = -148.07407407407408\n",
      "difference: -37 @Episode: 55 running: reward = -152.27272727272728\n",
      "difference: 33 @Episode: 56 running: reward = -148.48214285714286\n",
      "difference: -77 @Episode: 57 running: reward = -149.9298245614035\n",
      "difference: -53 @Episode: 58 running: reward = -150.72413793103448\n",
      "difference: -1 @Episode: 59 running: reward = -150.83050847457628\n",
      "difference: -47 @Episode: 60 running: reward = -150.53333333333333\n",
      "difference: -105 @Episode: 61 running: reward = -154.9672131147541\n",
      "difference: -101 @Episode: 62 running: reward = -156.17741935483872\n",
      "difference: -141 @Episode: 63 running: reward = -163.04761904761904\n",
      "difference: -79 @Episode: 64 running: reward = -163.59375\n",
      "difference: 37 @Episode: 65 running: reward = -160.1076923076923\n",
      "difference: 9 @Episode: 66 running: reward = -154.15151515151516\n",
      "difference: 13 @Episode: 67 running: reward = -152.92537313432837\n",
      "difference: -157 @Episode: 68 running: reward = -161.3235294117647\n",
      "difference: 37 @Episode: 69 running: reward = -156.04347826086956\n",
      "difference: -33 @Episode: 70 running: reward = -157.9\n",
      "difference: -21 @Episode: 71 running: reward = -155.74647887323943\n",
      "difference: -17 @Episode: 72 running: reward = -153.54166666666666\n",
      "difference: -133 @Episode: 73 running: reward = -160.64383561643837\n",
      "difference: -65 @Episode: 74 running: reward = -160.02702702702703\n",
      "difference: -7 @Episode: 75 running: reward = -163.04\n",
      "difference: -125 @Episode: 76 running: reward = -166.6184210526316\n",
      "difference: -129 @Episode: 77 running: reward = -173.74025974025975\n",
      "difference: -31 @Episode: 78 running: reward = -172.85897435897436\n",
      "difference: 47 @Episode: 79 running: reward = -166.22784810126583\n",
      "difference: -33 @Episode: 80 running: reward = -165.2625\n",
      "difference: -13 @Episode: 81 running: reward = -166.02469135802468\n",
      "difference: -23 @Episode: 82 running: reward = -164.0487804878049\n",
      "difference: -61 @Episode: 83 running: reward = -163.289156626506\n",
      "difference: -49 @Episode: 84 running: reward = -163.42857142857142\n",
      "difference: -23 @Episode: 85 running: reward = -166.43529411764706\n",
      "difference: -19 @Episode: 86 running: reward = -162.52325581395348\n",
      "difference: 83 @Episode: 87 running: reward = -158.93103448275863\n",
      "difference: -99 @Episode: 88 running: reward = -163.52272727272728\n",
      "difference: -65 @Episode: 89 running: reward = -163.22471910112358\n",
      "difference: -35 @Episode: 90 running: reward = -167.14444444444445\n",
      "difference: -39 @Episode: 91 running: reward = -167.64835164835165\n",
      "difference: -97 @Episode: 92 running: reward = -168.7826086956522\n",
      "difference: 29 @Episode: 93 running: reward = -162.38709677419354\n",
      "difference: -79 @Episode: 94 running: reward = -161.70212765957447\n",
      "difference: 9 @Episode: 95 running: reward = -161.78947368421052\n",
      "difference: -39 @Episode: 96 running: reward = -162.80208333333334\n",
      "difference: -29 @Episode: 97 running: reward = -160.41237113402062\n",
      "difference: -5 @Episode: 98 running: reward = -156.30612244897958\n",
      "difference: 5 @Episode: 99 running: reward = -154.22222222222223\n",
      "difference: -43 @Episode: 100 running: reward = -156.31\n",
      "difference: -71 @Episode: 101 running: reward = -159.85\n",
      "difference: -23 @Episode: 102 running: reward = -157.79\n",
      "difference: 43 @Episode: 103 running: reward = -153.74\n",
      "difference: 19 @Episode: 104 running: reward = -151.02\n",
      "difference: -5 @Episode: 105 running: reward = -146.86\n",
      "difference: -81 @Episode: 106 running: reward = -149.66\n",
      "difference: -33 @Episode: 107 running: reward = -148.84\n",
      "difference: -47 @Episode: 108 running: reward = -146.58\n",
      "difference: -9 @Episode: 109 running: reward = -142.58\n",
      "difference: -87 @Episode: 110 running: reward = -146.58\n",
      "difference: -83 @Episode: 111 running: reward = -149.34\n",
      "difference: -93 @Episode: 112 running: reward = -151.48\n",
      "difference: -83 @Episode: 113 running: reward = -151.28\n",
      "difference: -83 @Episode: 114 running: reward = -150.76\n",
      "difference: 13 @Episode: 115 running: reward = -149.15\n",
      "difference: 1 @Episode: 116 running: reward = -150.44\n",
      "difference: -41 @Episode: 117 running: reward = -155.54\n",
      "difference: -69 @Episode: 118 running: reward = -161.49\n",
      "difference: -65 @Episode: 119 running: reward = -166.57\n",
      "difference: 7 @Episode: 120 running: reward = -162.31\n",
      "difference: -61 @Episode: 121 running: reward = -157.59\n",
      "difference: -101 @Episode: 122 running: reward = -159.4\n",
      "difference: -77 @Episode: 123 running: reward = -160.75\n",
      "difference: -119 @Episode: 124 running: reward = -161.45\n",
      "difference: -93 @Episode: 125 running: reward = -159.86\n",
      "difference: -21 @Episode: 126 running: reward = -165.26\n",
      "difference: 5 @Episode: 127 running: reward = -162.88\n",
      "difference: -17 @Episode: 128 running: reward = -170.04\n",
      "difference: -35 @Episode: 129 running: reward = -167.35\n",
      "difference: -35 @Episode: 130 running: reward = -167.62\n",
      "difference: -71 @Episode: 131 running: reward = -162.36\n",
      "difference: -77 @Episode: 132 running: reward = -164.33\n",
      "difference: -5 @Episode: 133 running: reward = -164.21\n",
      "difference: -87 @Episode: 134 running: reward = -165.15\n",
      "difference: 67 @Episode: 135 running: reward = -158.84\n",
      "difference: -75 @Episode: 136 running: reward = -157.66\n",
      "difference: -73 @Episode: 137 running: reward = -162.46\n",
      "difference: 77 @Episode: 138 running: reward = -158.71\n",
      "difference: -63 @Episode: 139 running: reward = -157.61\n",
      "difference: -117 @Episode: 140 running: reward = -155.14\n",
      "difference: 7 @Episode: 141 running: reward = -153.78\n",
      "difference: -35 @Episode: 142 running: reward = -151.8\n",
      "difference: -75 @Episode: 143 running: reward = -151.36\n",
      "difference: -17 @Episode: 144 running: reward = -158.2\n",
      "difference: -49 @Episode: 145 running: reward = -157.95\n",
      "difference: -107 @Episode: 146 running: reward = -159.16\n",
      "difference: -99 @Episode: 147 running: reward = -158.78\n",
      "difference: 47 @Episode: 148 running: reward = -157.77\n",
      "difference: 7 @Episode: 149 running: reward = -156.16\n",
      "difference: 31 @Episode: 150 running: reward = -154.4\n",
      "difference: -43 @Episode: 151 running: reward = -149.09\n",
      "difference: -21 @Episode: 152 running: reward = -143.27\n",
      "difference: 83 @Episode: 153 running: reward = -141.9\n",
      "difference: -63 @Episode: 154 running: reward = -139.11\n",
      "difference: -47 @Episode: 155 running: reward = -136.0\n",
      "difference: -33 @Episode: 156 running: reward = -137.85\n",
      "difference: -109 @Episode: 157 running: reward = -138.47\n",
      "difference: 67 @Episode: 158 running: reward = -134.02\n",
      "difference: 17 @Episode: 159 running: reward = -131.29\n",
      "difference: -39 @Episode: 160 running: reward = -132.48\n",
      "difference: 31 @Episode: 161 running: reward = -127.15\n",
      "difference: 3 @Episode: 162 running: reward = -125.19\n",
      "difference: 61 @Episode: 163 running: reward = -119.38\n",
      "difference: -113 @Episode: 164 running: reward = -119.93\n",
      "difference: 63 @Episode: 165 running: reward = -119.61\n",
      "difference: -59 @Episode: 166 running: reward = -125.18\n",
      "difference: -55 @Episode: 167 running: reward = -127.08\n",
      "difference: -75 @Episode: 168 running: reward = -119.73\n",
      "difference: -49 @Episode: 169 running: reward = -119.86\n",
      "difference: -35 @Episode: 170 running: reward = -118.88\n",
      "difference: -31 @Episode: 171 running: reward = -118.48\n",
      "difference: -79 @Episode: 172 running: reward = -123.72\n",
      "difference: -95 @Episode: 173 running: reward = -121.13\n",
      "difference: -33 @Episode: 174 running: reward = -122.71\n",
      "difference: 91 @Episode: 175 running: reward = -114.63\n",
      "difference: -109 @Episode: 176 running: reward = -112.38\n",
      "difference: -35 @Episode: 177 running: reward = -105.73\n",
      "difference: -71 @Episode: 178 running: reward = -105.75\n",
      "difference: -47 @Episode: 179 running: reward = -111.21\n",
      "difference: 1 @Episode: 180 running: reward = -108.27\n",
      "difference: -45 @Episode: 181 running: reward = -107.79\n",
      "difference: 65 @Episode: 182 running: reward = -105.99\n",
      "difference: 23 @Episode: 183 running: reward = -103.62\n",
      "difference: -133 @Episode: 184 running: reward = -107.35\n",
      "difference: -37 @Episode: 185 running: reward = -103.6\n",
      "difference: -89 @Episode: 186 running: reward = -106.76\n",
      "difference: -35 @Episode: 187 running: reward = -108.85\n",
      "difference: -45 @Episode: 188 running: reward = -103.69\n",
      "difference: -9 @Episode: 189 running: reward = -103.19\n",
      "difference: -53 @Episode: 190 running: reward = -98.12\n",
      "difference: -49 @Episode: 191 running: reward = -97.41\n",
      "difference: 39 @Episode: 192 running: reward = -93.41\n",
      "difference: 71 @Episode: 193 running: reward = -93.11\n",
      "difference: -47 @Episode: 194 running: reward = -91.69\n",
      "difference: -17 @Episode: 195 running: reward = -91.33\n",
      "difference: -131 @Episode: 196 running: reward = -94.74\n",
      "difference: 11 @Episode: 197 running: reward = -93.09\n",
      "difference: -33 @Episode: 198 running: reward = -97.54\n",
      "difference: 25 @Episode: 199 running: reward = -93.84\n",
      "difference: -129 @Episode: 200 running: reward = -96.05\n",
      "difference: 23 @Episode: 201 running: reward = -90.19\n",
      "difference: 13 @Episode: 202 running: reward = -91.67\n",
      "difference: -57 @Episode: 203 running: reward = -93.59\n",
      "difference: -65 @Episode: 204 running: reward = -97.04\n",
      "difference: -47 @Episode: 205 running: reward = -99.64\n",
      "difference: 1 @Episode: 206 running: reward = -101.9\n",
      "difference: 11 @Episode: 207 running: reward = -101.9\n",
      "difference: 21 @Episode: 208 running: reward = -96.99\n",
      "difference: -41 @Episode: 209 running: reward = -97.08\n",
      "difference: 31 @Episode: 210 running: reward = -91.64\n",
      "difference: -105 @Episode: 211 running: reward = -90.02\n",
      "difference: -19 @Episode: 212 running: reward = -91.0\n",
      "difference: 43 @Episode: 213 running: reward = -88.03\n",
      "difference: -115 @Episode: 214 running: reward = -88.67\n",
      "difference: 27 @Episode: 215 running: reward = -89.96\n",
      "difference: 25 @Episode: 216 running: reward = -89.86\n",
      "difference: -59 @Episode: 217 running: reward = -88.48\n",
      "difference: -59 @Episode: 218 running: reward = -89.75\n",
      "difference: 35 @Episode: 219 running: reward = -87.92\n",
      "difference: -147 @Episode: 220 running: reward = -95.51\n",
      "difference: -75 @Episode: 221 running: reward = -98.12\n",
      "difference: -89 @Episode: 222 running: reward = -97.83\n",
      "difference: -143 @Episode: 223 running: reward = -100.71\n",
      "difference: -93 @Episode: 224 running: reward = -101.43\n",
      "difference: 19 @Episode: 225 running: reward = -100.22\n",
      "difference: -35 @Episode: 226 running: reward = -102.61\n",
      "difference: -53 @Episode: 227 running: reward = -106.49\n",
      "difference: -61 @Episode: 228 running: reward = -104.79\n",
      "difference: -39 @Episode: 229 running: reward = -106.87\n",
      "difference: -11 @Episode: 230 running: reward = -107.53\n",
      "difference: 7 @Episode: 231 running: reward = -110.11\n",
      "difference: 3 @Episode: 232 running: reward = -104.46\n",
      "difference: 15 @Episode: 233 running: reward = -101.33\n",
      "difference: -79 @Episode: 234 running: reward = -99.88\n",
      "difference: -17 @Episode: 235 running: reward = -102.22\n",
      "difference: -73 @Episode: 236 running: reward = -103.12\n",
      "difference: -3 @Episode: 237 running: reward = -98.41\n",
      "difference: 59 @Episode: 238 running: reward = -97.35\n",
      "difference: -53 @Episode: 239 running: reward = -99.03\n",
      "difference: -95 @Episode: 240 running: reward = -98.55\n",
      "difference: -59 @Episode: 241 running: reward = -98.81\n",
      "difference: 5 @Episode: 242 running: reward = -100.39\n",
      "difference: -45 @Episode: 243 running: reward = -103.87\n",
      "difference: 43 @Episode: 244 running: reward = -99.13\n",
      "difference: -71 @Episode: 245 running: reward = -101.28\n",
      "difference: -11 @Episode: 246 running: reward = -95.28\n",
      "difference: -37 @Episode: 247 running: reward = -94.53\n",
      "difference: -13 @Episode: 248 running: reward = -99.24\n",
      "difference: -39 @Episode: 249 running: reward = -95.81\n",
      "difference: -65 @Episode: 250 running: reward = -97.71\n",
      "difference: -99 @Episode: 251 running: reward = -100.97\n",
      "difference: 15 @Episode: 252 running: reward = -102.56\n",
      "difference: -21 @Episode: 253 running: reward = -103.36\n",
      "difference: -97 @Episode: 254 running: reward = -107.25\n",
      "difference: -43 @Episode: 255 running: reward = -109.57\n",
      "difference: -17 @Episode: 256 running: reward = -107.99\n",
      "difference: -17 @Episode: 257 running: reward = -103.2\n",
      "difference: -139 @Episode: 258 running: reward = -111.38\n",
      "difference: 87 @Episode: 259 running: reward = -108.61\n",
      "difference: 13 @Episode: 260 running: reward = -103.82\n",
      "difference: -61 @Episode: 261 running: reward = -107.17\n",
      "difference: -59 @Episode: 262 running: reward = -109.53\n",
      "difference: -93 @Episode: 263 running: reward = -113.17\n",
      "difference: -1 @Episode: 264 running: reward = -112.11\n",
      "difference: -39 @Episode: 265 running: reward = -115.96\n",
      "difference: -23 @Episode: 266 running: reward = -113.11\n",
      "difference: -131 @Episode: 267 running: reward = -114.62\n",
      "difference: -149 @Episode: 268 running: reward = -120.87\n",
      "difference: -115 @Episode: 269 running: reward = -125.53\n",
      "difference: -57 @Episode: 270 running: reward = -124.74\n",
      "difference: -17 @Episode: 271 running: reward = -125.89\n",
      "difference: 23 @Episode: 272 running: reward = -115.76\n",
      "difference: -73 @Episode: 273 running: reward = -116.18\n",
      "difference: -113 @Episode: 274 running: reward = -117.87\n",
      "difference: 3 @Episode: 275 running: reward = -119.79\n",
      "difference: -21 @Episode: 276 running: reward = -116.33\n",
      "difference: -61 @Episode: 277 running: reward = -117.53\n",
      "running reward: -117.53 at episode 277, frame count 10000\n",
      "difference: -63 @Episode: 278 running: reward = -119.49\n",
      "difference: -5 @Episode: 279 running: reward = -120.45\n",
      "difference: -65 @Episode: 280 running: reward = -127.11\n",
      "difference: -41 @Episode: 281 running: reward = -129.57\n",
      "difference: 15 @Episode: 282 running: reward = -131.58\n",
      "difference: 7 @Episode: 283 running: reward = -133.56\n",
      "difference: 75 @Episode: 284 running: reward = -122.25\n",
      "difference: -31 @Episode: 285 running: reward = -121.94\n",
      "difference: -125 @Episode: 286 running: reward = -126.18\n",
      "difference: -45 @Episode: 287 running: reward = -126.88\n",
      "difference: -23 @Episode: 288 running: reward = -127.75\n",
      "difference: -63 @Episode: 289 running: reward = -127.24\n",
      "difference: -75 @Episode: 290 running: reward = -129.01\n",
      "difference: 53 @Episode: 291 running: reward = -123.63\n",
      "difference: 37 @Episode: 292 running: reward = -125.49\n",
      "difference: -75 @Episode: 293 running: reward = -131.74\n",
      "difference: -79 @Episode: 294 running: reward = -132.01\n",
      "difference: 31 @Episode: 295 running: reward = -125.84\n",
      "difference: 5 @Episode: 296 running: reward = -121.16\n",
      "difference: 3 @Episode: 297 running: reward = -124.27\n",
      "difference: 27 @Episode: 298 running: reward = -122.63\n",
      "difference: -37 @Episode: 299 running: reward = -127.34\n",
      "difference: -103 @Episode: 300 running: reward = -125.46\n",
      "difference: -99 @Episode: 301 running: reward = -132.59\n",
      "difference: -85 @Episode: 302 running: reward = -134.02\n",
      "difference: -39 @Episode: 303 running: reward = -130.33\n",
      "difference: 21 @Episode: 304 running: reward = -125.69\n",
      "difference: -87 @Episode: 305 running: reward = -128.27\n",
      "difference: -101 @Episode: 306 running: reward = -128.55\n",
      "difference: -31 @Episode: 307 running: reward = -132.65\n",
      "difference: -91 @Episode: 308 running: reward = -140.45\n",
      "difference: 5 @Episode: 309 running: reward = -143.7\n",
      "difference: -77 @Episode: 310 running: reward = -144.43\n",
      "difference: -97 @Episode: 311 running: reward = -145.53\n",
      "difference: -15 @Episode: 312 running: reward = -147.31\n",
      "difference: -33 @Episode: 313 running: reward = -148.87\n",
      "difference: -5 @Episode: 314 running: reward = -145.29\n",
      "difference: 21 @Episode: 315 running: reward = -143.97\n",
      "difference: -37 @Episode: 316 running: reward = -142.82\n",
      "difference: -41 @Episode: 317 running: reward = -141.79\n",
      "difference: -59 @Episode: 318 running: reward = -140.63\n",
      "difference: 51 @Episode: 319 running: reward = -140.98\n",
      "difference: -119 @Episode: 320 running: reward = -139.08\n",
      "difference: -127 @Episode: 321 running: reward = -143.02\n",
      "difference: -35 @Episode: 322 running: reward = -140.77\n",
      "difference: -97 @Episode: 323 running: reward = -140.13\n",
      "difference: -99 @Episode: 324 running: reward = -140.62\n",
      "difference: -5 @Episode: 325 running: reward = -142.97\n",
      "difference: -157 @Episode: 326 running: reward = -145.39\n",
      "difference: -75 @Episode: 327 running: reward = -145.13\n",
      "difference: -35 @Episode: 328 running: reward = -145.88\n",
      "difference: -91 @Episode: 329 running: reward = -147.82\n",
      "difference: -37 @Episode: 330 running: reward = -147.09\n",
      "difference: -51 @Episode: 331 running: reward = -146.8\n",
      "difference: 53 @Episode: 332 running: reward = -147.51\n",
      "difference: -79 @Episode: 333 running: reward = -152.47\n",
      "difference: -27 @Episode: 334 running: reward = -150.53\n",
      "difference: -45 @Episode: 335 running: reward = -154.03\n",
      "difference: -85 @Episode: 336 running: reward = -153.25\n",
      "difference: -21 @Episode: 337 running: reward = -156.7\n",
      "difference: -35 @Episode: 338 running: reward = -160.02\n",
      "difference: -59 @Episode: 339 running: reward = -157.98\n",
      "difference: 27 @Episode: 340 running: reward = -151.87\n",
      "difference: -31 @Episode: 341 running: reward = -153.26\n",
      "difference: -57 @Episode: 342 running: reward = -153.7\n",
      "difference: 35 @Episode: 343 running: reward = -147.94\n",
      "difference: -129 @Episode: 344 running: reward = -157.56\n",
      "difference: -49 @Episode: 345 running: reward = -158.47\n",
      "difference: -133 @Episode: 346 running: reward = -168.01\n",
      "difference: 13 @Episode: 347 running: reward = -165.0\n",
      "difference: -105 @Episode: 348 running: reward = -166.57\n",
      "difference: -101 @Episode: 349 running: reward = -169.19\n",
      "difference: -91 @Episode: 350 running: reward = -171.59\n",
      "difference: -37 @Episode: 351 running: reward = -167.07\n",
      "difference: 9 @Episode: 352 running: reward = -165.95\n",
      "difference: -57 @Episode: 353 running: reward = -166.88\n",
      "difference: -101 @Episode: 354 running: reward = -169.81\n",
      "difference: -51 @Episode: 355 running: reward = -167.6\n",
      "difference: -17 @Episode: 356 running: reward = -168.07\n",
      "difference: -67 @Episode: 357 running: reward = -171.63\n",
      "difference: 29 @Episode: 358 running: reward = -165.19\n",
      "difference: -51 @Episode: 359 running: reward = -167.96\n",
      "difference: 77 @Episode: 360 running: reward = -167.72\n",
      "difference: -9 @Episode: 361 running: reward = -164.85\n",
      "difference: 25 @Episode: 362 running: reward = -160.58\n",
      "difference: 39 @Episode: 363 running: reward = -155.06\n",
      "difference: -1 @Episode: 364 running: reward = -153.01\n",
      "difference: -121 @Episode: 365 running: reward = -155.78\n",
      "difference: -53 @Episode: 366 running: reward = -156.09\n",
      "difference: -59 @Episode: 367 running: reward = -153.46\n",
      "difference: -107 @Episode: 368 running: reward = -152.24\n",
      "difference: -105 @Episode: 369 running: reward = -153.46\n",
      "difference: 27 @Episode: 370 running: reward = -153.79\n",
      "difference: -49 @Episode: 371 running: reward = -155.88\n",
      "difference: -81 @Episode: 372 running: reward = -163.54\n",
      "difference: 41 @Episode: 373 running: reward = -157.23\n",
      "difference: -37 @Episode: 374 running: reward = -151.38\n",
      "difference: -103 @Episode: 375 running: reward = -160.68\n",
      "difference: -87 @Episode: 376 running: reward = -162.75\n",
      "difference: -117 @Episode: 377 running: reward = -166.87\n",
      "difference: 35 @Episode: 378 running: reward = -164.1\n",
      "difference: -87 @Episode: 379 running: reward = -166.59\n",
      "difference: -73 @Episode: 380 running: reward = -164.53\n",
      "difference: -25 @Episode: 381 running: reward = -157.88\n",
      "difference: 11 @Episode: 382 running: reward = -158.53\n",
      "difference: -29 @Episode: 383 running: reward = -160.51\n",
      "difference: -81 @Episode: 384 running: reward = -170.24\n",
      "difference: -5 @Episode: 385 running: reward = -169.76\n",
      "difference: -109 @Episode: 386 running: reward = -166.43\n",
      "difference: -87 @Episode: 387 running: reward = -166.76\n",
      "difference: -13 @Episode: 388 running: reward = -167.29\n",
      "difference: -67 @Episode: 389 running: reward = -169.15\n",
      "difference: -93 @Episode: 390 running: reward = -171.12\n",
      "difference: 35 @Episode: 391 running: reward = -174.63\n",
      "difference: -87 @Episode: 392 running: reward = -179.16\n",
      "difference: -21 @Episode: 393 running: reward = -180.27\n",
      "difference: -87 @Episode: 394 running: reward = -183.93\n",
      "difference: -83 @Episode: 395 running: reward = -193.06\n",
      "difference: -5 @Episode: 396 running: reward = -191.09\n",
      "difference: -79 @Episode: 397 running: reward = -192.16\n",
      "difference: -143 @Episode: 398 running: reward = -196.95\n",
      "difference: -123 @Episode: 399 running: reward = -200.27\n",
      "difference: -93 @Episode: 400 running: reward = -199.87\n",
      "difference: -129 @Episode: 401 running: reward = -200.18\n",
      "difference: -25 @Episode: 402 running: reward = -198.3\n",
      "difference: -25 @Episode: 403 running: reward = -196.96\n",
      "difference: -105 @Episode: 404 running: reward = -202.59\n",
      "difference: 37 @Episode: 405 running: reward = -196.23\n",
      "difference: -15 @Episode: 406 running: reward = -195.74\n",
      "difference: -101 @Episode: 407 running: reward = -196.61\n",
      "difference: -1 @Episode: 408 running: reward = -191.69\n",
      "difference: -157 @Episode: 409 running: reward = -195.13\n",
      "difference: 45 @Episode: 410 running: reward = -191.57\n",
      "difference: -11 @Episode: 411 running: reward = -188.82\n",
      "difference: -75 @Episode: 412 running: reward = -187.78\n",
      "difference: 9 @Episode: 413 running: reward = -180.82\n",
      "difference: 25 @Episode: 414 running: reward = -179.82\n",
      "difference: -57 @Episode: 415 running: reward = -183.6\n",
      "difference: -55 @Episode: 416 running: reward = -184.64\n",
      "difference: -23 @Episode: 417 running: reward = -185.81\n",
      "difference: -65 @Episode: 418 running: reward = -186.02\n",
      "difference: -49 @Episode: 419 running: reward = -187.06\n",
      "difference: -111 @Episode: 420 running: reward = -189.54\n",
      "difference: -47 @Episode: 421 running: reward = -183.15\n",
      "difference: -47 @Episode: 422 running: reward = -181.69\n",
      "difference: 37 @Episode: 423 running: reward = -175.44\n",
      "difference: -37 @Episode: 424 running: reward = -170.97\n",
      "difference: -69 @Episode: 425 running: reward = -169.69\n",
      "difference: 41 @Episode: 426 running: reward = -160.4\n",
      "difference: -11 @Episode: 427 running: reward = -160.15\n",
      "difference: -77 @Episode: 428 running: reward = -159.9\n",
      "difference: -45 @Episode: 429 running: reward = -160.81\n",
      "difference: -49 @Episode: 430 running: reward = -161.71\n",
      "difference: -51 @Episode: 431 running: reward = -159.72\n",
      "difference: -99 @Episode: 432 running: reward = -164.75\n",
      "difference: -103 @Episode: 433 running: reward = -164.56\n",
      "difference: 27 @Episode: 434 running: reward = -168.13\n",
      "difference: -33 @Episode: 435 running: reward = -170.29\n",
      "difference: -87 @Episode: 436 running: reward = -171.47\n",
      "difference: -131 @Episode: 437 running: reward = -175.12\n",
      "difference: -1 @Episode: 438 running: reward = -174.02\n",
      "difference: -11 @Episode: 439 running: reward = -171.13\n",
      "difference: -65 @Episode: 440 running: reward = -176.58\n",
      "difference: -83 @Episode: 441 running: reward = -175.06\n",
      "difference: -123 @Episode: 442 running: reward = -177.01\n",
      "difference: -63 @Episode: 443 running: reward = -184.61\n",
      "difference: -117 @Episode: 444 running: reward = -182.36\n",
      "difference: -103 @Episode: 445 running: reward = -181.96\n",
      "difference: -61 @Episode: 446 running: reward = -180.43\n",
      "difference: -123 @Episode: 447 running: reward = -188.54\n",
      "difference: -5 @Episode: 448 running: reward = -183.97\n",
      "difference: -67 @Episode: 449 running: reward = -182.68\n",
      "difference: 9 @Episode: 450 running: reward = -179.12\n",
      "difference: -111 @Episode: 451 running: reward = -182.76\n",
      "difference: -1 @Episode: 452 running: reward = -179.9\n",
      "difference: -3 @Episode: 453 running: reward = -178.98\n",
      "difference: 63 @Episode: 454 running: reward = -168.87\n",
      "difference: -11 @Episode: 455 running: reward = -169.7\n",
      "difference: -107 @Episode: 456 running: reward = -172.22\n",
      "difference: -29 @Episode: 457 running: reward = -168.83\n",
      "difference: 7 @Episode: 458 running: reward = -170.24\n",
      "difference: -127 @Episode: 459 running: reward = -176.49\n",
      "difference: 19 @Episode: 460 running: reward = -179.48\n",
      "difference: -3 @Episode: 461 running: reward = -179.94\n",
      "difference: 7 @Episode: 462 running: reward = -181.12\n",
      "difference: 17 @Episode: 463 running: reward = -179.52\n",
      "difference: 89 @Episode: 464 running: reward = -176.75\n",
      "difference: 63 @Episode: 465 running: reward = -168.68\n",
      "difference: -81 @Episode: 466 running: reward = -173.96\n",
      "difference: 3 @Episode: 467 running: reward = -173.57\n",
      "difference: -9 @Episode: 468 running: reward = -169.86\n",
      "difference: -31 @Episode: 469 running: reward = -169.53\n",
      "difference: -11 @Episode: 470 running: reward = -170.73\n",
      "difference: -79 @Episode: 471 running: reward = -169.89\n",
      "difference: 37 @Episode: 472 running: reward = -166.27\n",
      "difference: -37 @Episode: 473 running: reward = -169.24\n",
      "difference: -25 @Episode: 474 running: reward = -171.44\n",
      "difference: -109 @Episode: 475 running: reward = -167.18\n",
      "difference: -87 @Episode: 476 running: reward = -171.1\n",
      "difference: -1 @Episode: 477 running: reward = -165.5\n",
      "difference: -17 @Episode: 478 running: reward = -164.34\n",
      "difference: -57 @Episode: 479 running: reward = -158.6\n",
      "difference: -93 @Episode: 480 running: reward = -161.84\n",
      "difference: -55 @Episode: 481 running: reward = -164.02\n",
      "difference: -9 @Episode: 482 running: reward = -164.4\n",
      "difference: -57 @Episode: 483 running: reward = -162.98\n",
      "difference: -91 @Episode: 484 running: reward = -163.25\n",
      "difference: -157 @Episode: 485 running: reward = -170.06\n",
      "difference: -147 @Episode: 486 running: reward = -173.72\n",
      "difference: -43 @Episode: 487 running: reward = -173.25\n",
      "difference: -29 @Episode: 488 running: reward = -170.85\n",
      "difference: -71 @Episode: 489 running: reward = -170.08\n",
      "difference: -35 @Episode: 490 running: reward = -170.09\n",
      "difference: -59 @Episode: 491 running: reward = -171.37\n",
      "difference: -55 @Episode: 492 running: reward = -166.96\n",
      "difference: 51 @Episode: 493 running: reward = -160.92\n",
      "difference: -85 @Episode: 494 running: reward = -161.32\n",
      "difference: 13 @Episode: 495 running: reward = -159.64\n",
      "difference: -33 @Episode: 496 running: reward = -160.8\n",
      "difference: -121 @Episode: 497 running: reward = -164.23\n",
      "difference: 15 @Episode: 498 running: reward = -161.4\n",
      "difference: -43 @Episode: 499 running: reward = -161.42\n",
      "difference: 41 @Episode: 500 running: reward = -155.25\n",
      "difference: -93 @Episode: 501 running: reward = -150.88\n",
      "difference: -83 @Episode: 502 running: reward = -152.3\n",
      "difference: -3 @Episode: 503 running: reward = -155.57\n",
      "difference: -15 @Episode: 504 running: reward = -149.05\n",
      "difference: -73 @Episode: 505 running: reward = -151.19\n",
      "difference: -67 @Episode: 506 running: reward = -151.83\n",
      "difference: 55 @Episode: 507 running: reward = -144.31\n",
      "difference: -19 @Episode: 508 running: reward = -144.57\n",
      "difference: -39 @Episode: 509 running: reward = -143.46\n",
      "difference: -11 @Episode: 510 running: reward = -144.35\n",
      "difference: -35 @Episode: 511 running: reward = -147.11\n",
      "difference: -131 @Episode: 512 running: reward = -147.61\n",
      "difference: -157 @Episode: 513 running: reward = -157.45\n",
      "difference: 21 @Episode: 514 running: reward = -156.73\n",
      "difference: 45 @Episode: 515 running: reward = -154.57\n",
      "difference: -51 @Episode: 516 running: reward = -156.52\n",
      "difference: 5 @Episode: 517 running: reward = -152.49\n",
      "difference: -11 @Episode: 518 running: reward = -145.76\n",
      "difference: -31 @Episode: 519 running: reward = -147.15\n",
      "difference: -7 @Episode: 520 running: reward = -139.23\n",
      "difference: 23 @Episode: 521 running: reward = -136.06\n",
      "difference: -89 @Episode: 522 running: reward = -138.95\n",
      "difference: -33 @Episode: 523 running: reward = -140.36\n",
      "difference: -15 @Episode: 524 running: reward = -140.76\n",
      "difference: -35 @Episode: 525 running: reward = -138.64\n",
      "difference: -89 @Episode: 526 running: reward = -145.71\n",
      "difference: 37 @Episode: 527 running: reward = -141.85\n",
      "difference: -157 @Episode: 528 running: reward = -147.64\n",
      "difference: -17 @Episode: 529 running: reward = -141.32\n",
      "difference: -157 @Episode: 530 running: reward = -147.17\n",
      "difference: -43 @Episode: 531 running: reward = -144.73\n",
      "difference: -69 @Episode: 532 running: reward = -143.78\n",
      "difference: -35 @Episode: 533 running: reward = -144.81\n",
      "difference: 49 @Episode: 534 running: reward = -143.21\n",
      "difference: 9 @Episode: 535 running: reward = -136.58\n",
      "difference: 43 @Episode: 536 running: reward = -133.74\n",
      "difference: -97 @Episode: 537 running: reward = -130.52\n",
      "difference: -131 @Episode: 538 running: reward = -135.93\n",
      "difference: -21 @Episode: 539 running: reward = -141.72\n",
      "difference: -157 @Episode: 540 running: reward = -145.27\n",
      "difference: -41 @Episode: 541 running: reward = -143.23\n",
      "difference: -107 @Episode: 542 running: reward = -144.81\n",
      "difference: -109 @Episode: 543 running: reward = -144.7\n",
      "difference: -31 @Episode: 544 running: reward = -142.33\n",
      "difference: -157 @Episode: 545 running: reward = -145.47\n",
      "difference: -39 @Episode: 546 running: reward = -141.42\n",
      "difference: -49 @Episode: 547 running: reward = -137.13\n",
      "difference: -37 @Episode: 548 running: reward = -139.22\n",
      "difference: 101 @Episode: 549 running: reward = -134.29\n",
      "difference: -33 @Episode: 550 running: reward = -135.45\n",
      "difference: -23 @Episode: 551 running: reward = -132.45\n",
      "difference: -75 @Episode: 552 running: reward = -137.01\n",
      "difference: -91 @Episode: 553 running: reward = -139.37\n",
      "difference: -83 @Episode: 554 running: reward = -145.11\n",
      "difference: -73 @Episode: 555 running: reward = -143.61\n",
      "running reward: -143.61 at episode 555, frame count 20000\n",
      "difference: 15 @Episode: 556 running: reward = -140.99\n",
      "difference: -27 @Episode: 557 running: reward = -146.7\n",
      "difference: -7 @Episode: 558 running: reward = -147.96\n",
      "difference: -85 @Episode: 559 running: reward = -145.28\n",
      "difference: -87 @Episode: 560 running: reward = -149.65\n",
      "difference: -23 @Episode: 561 running: reward = -151.82\n",
      "difference: 95 @Episode: 562 running: reward = -148.41\n",
      "difference: 73 @Episode: 563 running: reward = -147.12\n",
      "difference: -11 @Episode: 564 running: reward = -148.36\n",
      "difference: -145 @Episode: 565 running: reward = -155.85\n",
      "difference: -43 @Episode: 566 running: reward = -155.95\n",
      "difference: -95 @Episode: 567 running: reward = -156.51\n",
      "difference: -103 @Episode: 568 running: reward = -161.71\n",
      "difference: -33 @Episode: 569 running: reward = -157.98\n",
      "difference: -43 @Episode: 570 running: reward = -156.82\n",
      "difference: 83 @Episode: 571 running: reward = -149.67\n",
      "difference: -59 @Episode: 572 running: reward = -151.4\n",
      "difference: -39 @Episode: 573 running: reward = -153.22\n",
      "difference: -89 @Episode: 574 running: reward = -158.54\n",
      "difference: 55 @Episode: 575 running: reward = -153.85\n",
      "difference: -23 @Episode: 576 running: reward = -153.17\n",
      "difference: -61 @Episode: 577 running: reward = -153.44\n",
      "difference: -79 @Episode: 578 running: reward = -156.09\n",
      "difference: -5 @Episode: 579 running: reward = -154.84\n",
      "difference: -73 @Episode: 580 running: reward = -152.68\n",
      "difference: 25 @Episode: 581 running: reward = -150.99\n",
      "difference: -31 @Episode: 582 running: reward = -150.02\n",
      "difference: -47 @Episode: 583 running: reward = -149.01\n",
      "difference: 1 @Episode: 584 running: reward = -142.48\n",
      "difference: -97 @Episode: 585 running: reward = -138.69\n",
      "difference: 51 @Episode: 586 running: reward = -130.85\n",
      "difference: -27 @Episode: 587 running: reward = -130.64\n",
      "difference: -81 @Episode: 588 running: reward = -135.19\n",
      "difference: -3 @Episode: 589 running: reward = -135.63\n",
      "difference: 17 @Episode: 590 running: reward = -129.7\n",
      "difference: 9 @Episode: 591 running: reward = -131.13\n",
      "difference: -69 @Episode: 592 running: reward = -135.11\n",
      "difference: 5 @Episode: 593 running: reward = -137.05\n",
      "difference: -53 @Episode: 594 running: reward = -135.14\n",
      "difference: -35 @Episode: 595 running: reward = -132.14\n",
      "difference: -125 @Episode: 596 running: reward = -136.33\n",
      "difference: 11 @Episode: 597 running: reward = -128.36\n",
      "difference: -95 @Episode: 598 running: reward = -127.4\n",
      "difference: -127 @Episode: 599 running: reward = -130.33\n",
      "difference: -31 @Episode: 600 running: reward = -137.73\n",
      "difference: 11 @Episode: 601 running: reward = -135.33\n",
      "difference: -13 @Episode: 602 running: reward = -133.18\n",
      "difference: 19 @Episode: 603 running: reward = -132.69\n",
      "difference: 43 @Episode: 604 running: reward = -131.07\n",
      "difference: -23 @Episode: 605 running: reward = -131.27\n",
      "difference: -77 @Episode: 606 running: reward = -131.65\n",
      "difference: 11 @Episode: 607 running: reward = -131.59\n",
      "difference: -67 @Episode: 608 running: reward = -136.29\n",
      "difference: 67 @Episode: 609 running: reward = -130.26\n",
      "difference: -119 @Episode: 610 running: reward = -137.66\n",
      "difference: -113 @Episode: 611 running: reward = -140.74\n",
      "difference: -75 @Episode: 612 running: reward = -140.95\n",
      "difference: 57 @Episode: 613 running: reward = -135.48\n",
      "difference: -57 @Episode: 614 running: reward = -135.56\n",
      "difference: -97 @Episode: 615 running: reward = -139.99\n",
      "difference: -127 @Episode: 616 running: reward = -144.09\n",
      "difference: -7 @Episode: 617 running: reward = -146.27\n",
      "difference: 1 @Episode: 618 running: reward = -150.71\n",
      "difference: -77 @Episode: 619 running: reward = -150.65\n",
      "difference: 69 @Episode: 620 running: reward = -146.94\n",
      "difference: -33 @Episode: 621 running: reward = -153.51\n",
      "difference: -107 @Episode: 622 running: reward = -155.82\n",
      "difference: -7 @Episode: 623 running: reward = -153.4\n",
      "difference: 37 @Episode: 624 running: reward = -152.45\n",
      "difference: -67 @Episode: 625 running: reward = -155.04\n",
      "difference: -9 @Episode: 626 running: reward = -150.75\n",
      "difference: 29 @Episode: 627 running: reward = -151.77\n",
      "difference: -51 @Episode: 628 running: reward = -149.86\n",
      "difference: -105 @Episode: 629 running: reward = -157.57\n",
      "difference: 3 @Episode: 630 running: reward = -152.82\n",
      "difference: 27 @Episode: 631 running: reward = -151.8\n",
      "difference: -103 @Episode: 632 running: reward = -151.3\n",
      "difference: 73 @Episode: 633 running: reward = -141.27\n",
      "difference: -69 @Episode: 634 running: reward = -143.74\n",
      "difference: -33 @Episode: 635 running: reward = -145.42\n",
      "difference: 11 @Episode: 636 running: reward = -143.84\n",
      "difference: 69 @Episode: 637 running: reward = -141.47\n",
      "difference: -45 @Episode: 638 running: reward = -135.81\n",
      "difference: -89 @Episode: 639 running: reward = -137.09\n",
      "difference: -61 @Episode: 640 running: reward = -132.14\n",
      "difference: 31 @Episode: 641 running: reward = -134.29\n",
      "difference: -71 @Episode: 642 running: reward = -128.93\n",
      "difference: -67 @Episode: 643 running: reward = -124.09\n",
      "difference: -37 @Episode: 644 running: reward = -123.88\n",
      "difference: -19 @Episode: 645 running: reward = -116.53\n",
      "difference: 13 @Episode: 646 running: reward = -114.26\n",
      "difference: -15 @Episode: 647 running: reward = -112.48\n",
      "difference: -23 @Episode: 648 running: reward = -109.44\n",
      "difference: -41 @Episode: 649 running: reward = -115.6\n",
      "difference: -59 @Episode: 650 running: reward = -119.32\n",
      "difference: -5 @Episode: 651 running: reward = -116.54\n",
      "difference: -21 @Episode: 652 running: reward = -113.42\n",
      "difference: 55 @Episode: 653 running: reward = -111.04\n",
      "difference: -35 @Episode: 654 running: reward = -111.77\n",
      "difference: -93 @Episode: 655 running: reward = -115.39\n",
      "difference: -123 @Episode: 656 running: reward = -118.44\n",
      "difference: -101 @Episode: 657 running: reward = -117.01\n",
      "difference: 25 @Episode: 658 running: reward = -113.4\n",
      "difference: 21 @Episode: 659 running: reward = -110.38\n",
      "difference: -79 @Episode: 660 running: reward = -106.33\n",
      "difference: -137 @Episode: 661 running: reward = -109.61\n",
      "difference: -85 @Episode: 662 running: reward = -116.62\n",
      "difference: 17 @Episode: 663 running: reward = -120.75\n",
      "difference: -97 @Episode: 664 running: reward = -125.19\n",
      "difference: -5 @Episode: 665 running: reward = -121.73\n",
      "difference: -101 @Episode: 666 running: reward = -119.94\n",
      "difference: -35 @Episode: 667 running: reward = -117.59\n",
      "difference: -145 @Episode: 668 running: reward = -115.27\n",
      "difference: -93 @Episode: 669 running: reward = -117.54\n",
      "difference: 51 @Episode: 670 running: reward = -114.7\n",
      "difference: 5 @Episode: 671 running: reward = -117.29\n",
      "difference: 5 @Episode: 672 running: reward = -115.34\n",
      "difference: -29 @Episode: 673 running: reward = -114.85\n",
      "difference: -105 @Episode: 674 running: reward = -111.62\n",
      "difference: -41 @Episode: 675 running: reward = -114.76\n",
      "difference: -15 @Episode: 676 running: reward = -112.43\n",
      "difference: -59 @Episode: 677 running: reward = -111.71\n",
      "difference: -39 @Episode: 678 running: reward = -110.37\n",
      "difference: -35 @Episode: 679 running: reward = -113.04\n",
      "difference: -85 @Episode: 680 running: reward = -115.83\n",
      "difference: -39 @Episode: 681 running: reward = -118.5\n",
      "difference: -105 @Episode: 682 running: reward = -123.57\n",
      "difference: -19 @Episode: 683 running: reward = -124.9\n",
      "difference: -21 @Episode: 684 running: reward = -126.94\n",
      "difference: -111 @Episode: 685 running: reward = -127.58\n",
      "difference: -55 @Episode: 686 running: reward = -132.59\n",
      "difference: 21 @Episode: 687 running: reward = -130.42\n",
      "difference: -39 @Episode: 688 running: reward = -128.22\n",
      "difference: -15 @Episode: 689 running: reward = -124.34\n",
      "difference: -45 @Episode: 690 running: reward = -126.43\n",
      "difference: -31 @Episode: 691 running: reward = -126.76\n",
      "difference: -109 @Episode: 692 running: reward = -126.39\n",
      "difference: -53 @Episode: 693 running: reward = -129.87\n",
      "difference: -89 @Episode: 694 running: reward = -131.0\n",
      "difference: -35 @Episode: 695 running: reward = -134.3\n",
      "difference: -23 @Episode: 696 running: reward = -127.41\n",
      "difference: 71 @Episode: 697 running: reward = -129.54\n",
      "difference: -43 @Episode: 698 running: reward = -130.91\n",
      "difference: -119 @Episode: 699 running: reward = -128.2\n",
      "difference: -3 @Episode: 700 running: reward = -125.53\n",
      "difference: -93 @Episode: 701 running: reward = -128.99\n",
      "difference: 37 @Episode: 702 running: reward = -128.52\n",
      "difference: -49 @Episode: 703 running: reward = -130.15\n",
      "difference: -37 @Episode: 704 running: reward = -135.04\n",
      "difference: 15 @Episode: 705 running: reward = -132.7\n",
      "difference: 33 @Episode: 706 running: reward = -125.32\n",
      "difference: -39 @Episode: 707 running: reward = -127.94\n",
      "difference: -9 @Episode: 708 running: reward = -124.49\n",
      "difference: 77 @Episode: 709 running: reward = -123.43\n",
      "difference: -25 @Episode: 710 running: reward = -119.12\n",
      "difference: -71 @Episode: 711 running: reward = -113.33\n",
      "difference: -57 @Episode: 712 running: reward = -113.33\n",
      "difference: 35 @Episode: 713 running: reward = -109.15\n",
      "difference: 19 @Episode: 714 running: reward = -109.11\n",
      "difference: -67 @Episode: 715 running: reward = -105.41\n",
      "difference: -139 @Episode: 716 running: reward = -103.57\n",
      "difference: 7 @Episode: 717 running: reward = -101.77\n",
      "difference: -65 @Episode: 718 running: reward = -100.04\n",
      "difference: -99 @Episode: 719 running: reward = -100.02\n",
      "difference: -157 @Episode: 720 running: reward = -111.03\n",
      "difference: -91 @Episode: 721 running: reward = -111.8\n",
      "difference: -33 @Episode: 722 running: reward = -107.61\n",
      "difference: -15 @Episode: 723 running: reward = -111.9\n",
      "difference: -137 @Episode: 724 running: reward = -116.21\n",
      "difference: 43 @Episode: 725 running: reward = -112.66\n",
      "difference: 7 @Episode: 726 running: reward = -112.25\n",
      "difference: -39 @Episode: 727 running: reward = -115.65\n",
      "difference: -5 @Episode: 728 running: reward = -109.79\n",
      "difference: -43 @Episode: 729 running: reward = -105.0\n",
      "difference: 13 @Episode: 730 running: reward = -102.97\n",
      "difference: -67 @Episode: 731 running: reward = -110.87\n",
      "difference: -119 @Episode: 732 running: reward = -114.78\n",
      "difference: -105 @Episode: 733 running: reward = -124.31\n",
      "difference: 41 @Episode: 734 running: reward = -119.23\n",
      "difference: -73 @Episode: 735 running: reward = -122.46\n",
      "difference: -79 @Episode: 736 running: reward = -126.49\n",
      "difference: -113 @Episode: 737 running: reward = -130.64\n",
      "difference: -33 @Episode: 738 running: reward = -132.76\n",
      "difference: -93 @Episode: 739 running: reward = -132.76\n",
      "difference: -117 @Episode: 740 running: reward = -135.23\n",
      "difference: -65 @Episode: 741 running: reward = -137.64\n",
      "difference: -59 @Episode: 742 running: reward = -137.06\n",
      "difference: -17 @Episode: 743 running: reward = -136.33\n",
      "difference: -33 @Episode: 744 running: reward = -138.58\n",
      "difference: -45 @Episode: 745 running: reward = -141.04\n",
      "difference: 17 @Episode: 746 running: reward = -143.82\n",
      "difference: 9 @Episode: 747 running: reward = -145.91\n",
      "difference: 53 @Episode: 748 running: reward = -145.78\n",
      "difference: -67 @Episode: 749 running: reward = -147.4\n",
      "difference: -151 @Episode: 750 running: reward = -148.38\n",
      "difference: 29 @Episode: 751 running: reward = -153.99\n",
      "difference: 55 @Episode: 752 running: reward = -155.44\n",
      "difference: -45 @Episode: 753 running: reward = -156.67\n",
      "difference: -157 @Episode: 754 running: reward = -160.83\n",
      "difference: 51 @Episode: 755 running: reward = -156.83\n",
      "difference: -27 @Episode: 756 running: reward = -152.53\n",
      "difference: 37 @Episode: 757 running: reward = -145.87\n",
      "difference: -141 @Episode: 758 running: reward = -154.26\n",
      "difference: 73 @Episode: 759 running: reward = -154.2\n",
      "difference: 69 @Episode: 760 running: reward = -149.03\n",
      "difference: -67 @Episode: 761 running: reward = -148.94\n",
      "difference: 29 @Episode: 762 running: reward = -142.77\n",
      "difference: -37 @Episode: 763 running: reward = -141.68\n",
      "difference: -49 @Episode: 764 running: reward = -138.4\n",
      "difference: -31 @Episode: 765 running: reward = -138.15\n",
      "difference: 9 @Episode: 766 running: reward = -131.73\n",
      "difference: -83 @Episode: 767 running: reward = -134.12\n",
      "difference: -51 @Episode: 768 running: reward = -131.73\n",
      "difference: -39 @Episode: 769 running: reward = -130.52\n",
      "difference: -49 @Episode: 770 running: reward = -132.88\n",
      "difference: -57 @Episode: 771 running: reward = -137.64\n",
      "difference: -37 @Episode: 772 running: reward = -140.99\n",
      "difference: -121 @Episode: 773 running: reward = -142.16\n",
      "difference: -3 @Episode: 774 running: reward = -137.21\n",
      "difference: -157 @Episode: 775 running: reward = -142.44\n",
      "difference: -15 @Episode: 776 running: reward = -142.21\n",
      "difference: -83 @Episode: 777 running: reward = -143.83\n",
      "difference: -59 @Episode: 778 running: reward = -144.84\n",
      "difference: -45 @Episode: 779 running: reward = -146.25\n",
      "difference: -63 @Episode: 780 running: reward = -140.37\n",
      "difference: -67 @Episode: 781 running: reward = -140.7\n",
      "difference: -17 @Episode: 782 running: reward = -139.85\n",
      "difference: -55 @Episode: 783 running: reward = -139.62\n",
      "difference: 85 @Episode: 784 running: reward = -136.77\n",
      "difference: -85 @Episode: 785 running: reward = -137.98\n",
      "difference: -157 @Episode: 786 running: reward = -141.93\n",
      "difference: 1 @Episode: 787 running: reward = -139.94\n",
      "difference: -65 @Episode: 788 running: reward = -139.96\n",
      "difference: -101 @Episode: 789 running: reward = -146.09\n",
      "difference: -137 @Episode: 790 running: reward = -150.13\n",
      "difference: -95 @Episode: 791 running: reward = -152.98\n",
      "difference: -107 @Episode: 792 running: reward = -151.88\n",
      "difference: -149 @Episode: 793 running: reward = -155.65\n",
      "difference: -81 @Episode: 794 running: reward = -155.27\n",
      "difference: -51 @Episode: 795 running: reward = -153.93\n",
      "difference: -7 @Episode: 796 running: reward = -156.54\n",
      "difference: -39 @Episode: 797 running: reward = -157.82\n",
      "difference: -63 @Episode: 798 running: reward = -159.7\n",
      "difference: 31 @Episode: 799 running: reward = -153.96\n",
      "difference: 45 @Episode: 800 running: reward = -153.59\n",
      "difference: -5 @Episode: 801 running: reward = -153.75\n",
      "difference: -51 @Episode: 802 running: reward = -156.39\n",
      "difference: 7 @Episode: 803 running: reward = -153.98\n",
      "difference: -131 @Episode: 804 running: reward = -155.82\n",
      "difference: -51 @Episode: 805 running: reward = -156.32\n",
      "difference: 59 @Episode: 806 running: reward = -158.44\n",
      "difference: -25 @Episode: 807 running: reward = -156.89\n",
      "difference: -85 @Episode: 808 running: reward = -159.92\n",
      "difference: -59 @Episode: 809 running: reward = -163.02\n",
      "difference: -7 @Episode: 810 running: reward = -164.06\n",
      "difference: -95 @Episode: 811 running: reward = -165.96\n",
      "difference: -137 @Episode: 812 running: reward = -167.27\n",
      "difference: -83 @Episode: 813 running: reward = -173.29\n",
      "difference: -81 @Episode: 814 running: reward = -176.49\n",
      "difference: -137 @Episode: 815 running: reward = -179.86\n",
      "difference: -105 @Episode: 816 running: reward = -178.82\n",
      "difference: -9 @Episode: 817 running: reward = -182.25\n",
      "difference: 17 @Episode: 818 running: reward = -180.03\n",
      "difference: -101 @Episode: 819 running: reward = -180.19\n",
      "difference: 31 @Episode: 820 running: reward = -173.48\n",
      "difference: -95 @Episode: 821 running: reward = -172.18\n",
      "difference: -129 @Episode: 822 running: reward = -176.03\n",
      "difference: -77 @Episode: 823 running: reward = -177.27\n",
      "difference: 5 @Episode: 824 running: reward = -176.38\n",
      "difference: -91 @Episode: 825 running: reward = -182.39\n",
      "difference: -101 @Episode: 826 running: reward = -185.16\n",
      "difference: 55 @Episode: 827 running: reward = -183.17\n",
      "difference: -3 @Episode: 828 running: reward = -183.93\n",
      "difference: -129 @Episode: 829 running: reward = -187.51\n",
      "difference: 39 @Episode: 830 running: reward = -184.16\n",
      "difference: 39 @Episode: 831 running: reward = -177.53\n",
      "difference: 59 @Episode: 832 running: reward = -171.49\n",
      "difference: -57 @Episode: 833 running: reward = -169.59\n",
      "running reward: -169.59 at episode 833, frame count 30000\n",
      "difference: -7 @Episode: 834 running: reward = -170.73\n",
      "difference: 79 @Episode: 835 running: reward = -164.26\n",
      "difference: -9 @Episode: 836 running: reward = -162.41\n",
      "difference: -21 @Episode: 837 running: reward = -160.99\n",
      "difference: -41 @Episode: 838 running: reward = -160.3\n",
      "difference: -27 @Episode: 839 running: reward = -154.94\n",
      "difference: -7 @Episode: 840 running: reward = -153.72\n",
      "difference: 45 @Episode: 841 running: reward = -151.35\n",
      "difference: -67 @Episode: 842 running: reward = -152.39\n",
      "difference: -99 @Episode: 843 running: reward = -154.08\n",
      "difference: 27 @Episode: 844 running: reward = -147.51\n",
      "difference: -73 @Episode: 845 running: reward = -147.3\n",
      "difference: -31 @Episode: 846 running: reward = -149.33\n",
      "difference: -81 @Episode: 847 running: reward = -152.45\n",
      "difference: 35 @Episode: 848 running: reward = -149.15\n",
      "difference: -85 @Episode: 849 running: reward = -147.56\n",
      "difference: -65 @Episode: 850 running: reward = -145.54\n",
      "difference: -57 @Episode: 851 running: reward = -147.73\n",
      "difference: -57 @Episode: 852 running: reward = -150.75\n",
      "difference: -79 @Episode: 853 running: reward = -151.82\n",
      "difference: 27 @Episode: 854 running: reward = -146.19\n",
      "difference: -11 @Episode: 855 running: reward = -147.52\n",
      "difference: -79 @Episode: 856 running: reward = -153.08\n",
      "difference: -11 @Episode: 857 running: reward = -157.41\n",
      "difference: -133 @Episode: 858 running: reward = -155.4\n",
      "difference: -77 @Episode: 859 running: reward = -155.88\n",
      "difference: -19 @Episode: 860 running: reward = -164.18\n",
      "difference: -61 @Episode: 861 running: reward = -159.9\n",
      "difference: 43 @Episode: 862 running: reward = -160.32\n",
      "difference: -85 @Episode: 863 running: reward = -162.85\n",
      "difference: 9 @Episode: 864 running: reward = -160.27\n",
      "difference: 11 @Episode: 865 running: reward = -158.02\n",
      "difference: -61 @Episode: 866 running: reward = -165.05\n",
      "difference: 35 @Episode: 867 running: reward = -162.71\n",
      "difference: -65 @Episode: 868 running: reward = -162.35\n",
      "difference: -81 @Episode: 869 running: reward = -163.81\n",
      "difference: 1 @Episode: 870 running: reward = -161.66\n",
      "difference: -29 @Episode: 871 running: reward = -159.23\n",
      "difference: -17 @Episode: 872 running: reward = -155.2\n",
      "difference: 7 @Episode: 873 running: reward = -152.9\n",
      "difference: -25 @Episode: 874 running: reward = -156.02\n",
      "difference: -81 @Episode: 875 running: reward = -152.62\n",
      "difference: -27 @Episode: 876 running: reward = -154.75\n",
      "difference: -17 @Episode: 877 running: reward = -153.75\n",
      "difference: -81 @Episode: 878 running: reward = -153.25\n",
      "difference: -49 @Episode: 879 running: reward = -151.92\n",
      "difference: -21 @Episode: 880 running: reward = -153.25\n",
      "difference: -53 @Episode: 881 running: reward = -153.71\n",
      "difference: 55 @Episode: 882 running: reward = -147.18\n",
      "difference: -33 @Episode: 883 running: reward = -145.68\n",
      "difference: -7 @Episode: 884 running: reward = -147.49\n",
      "difference: -21 @Episode: 885 running: reward = -142.61\n",
      "difference: 19 @Episode: 886 running: reward = -135.8\n",
      "difference: -73 @Episode: 887 running: reward = -140.71\n",
      "difference: -49 @Episode: 888 running: reward = -142.4\n",
      "difference: -21 @Episode: 889 running: reward = -139.29\n"
     ]
    }
   ],
   "source": [
    "env = Environment()\n",
    "\n",
    "# In the Deepmind paper they use RMSProp however then Adam optimizer\n",
    "# improves training time\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "\n",
    "# Experience replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "episode_reward_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "move_count = 0\n",
    "# Number of frames to take random action and observe output\n",
    "epsilon_random_frames = 50000\n",
    "# Number of frames for exploration\n",
    "epsilon_greedy_frames = 1000000.0\n",
    "# Maximum replay length\n",
    "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
    "max_memory_length = 100000\n",
    "# Train the model after 4 actions\n",
    "update_after_actions = 4\n",
    "# How often to update the target network\n",
    "update_target_network = 10000\n",
    "# Using huber loss for stability\n",
    "loss_function = keras.losses.Huber()\n",
    "\n",
    "\n",
    "while True:  # Run until solved\n",
    "    env.new_game() # Start Game (80,)\n",
    "    state = env.state()\n",
    "    episode_reward = 0\n",
    "\n",
    "    for time_step in range(1, max_steps_per_episode):\n",
    "        # env.render(); Adding this line would show the attempts\n",
    "        # of the agent in a pop up window.\n",
    "        move_count += 1\n",
    "\n",
    "        # Use epsilon-greedy for exploration\n",
    "        if move_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            # Take random action\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # Predict action Q-values\n",
    "            # From environment state\n",
    "            state_tensor = tf.convert_to_tensor(state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            action_probs = model(state_tensor, training=False)\n",
    "            # Take best action\n",
    "            action = tf.argmax(action_probs[0]).numpy()\n",
    "\n",
    "        # Decay probability of taking random action\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "        # Apply the sampled action in our environment\n",
    "        reward, done = env.step(action)\n",
    "        state_next = env.state()\n",
    "        # state_next = np.array(state_next)\n",
    "\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Save actions and states in replay buffer\n",
    "        action_history.append(action)\n",
    "        state_history.append(state)\n",
    "        state_next_history.append(state_next)\n",
    "        done_history.append(done)\n",
    "        rewards_history.append(reward)\n",
    "        state = state_next\n",
    "\n",
    "        # Update every fourth frame and once batch size is over 32\n",
    "        if move_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "\n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
    "\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = np.array([state_history[i] for i in indices])\n",
    "            state_next_sample = np.array([state_next_history[i] for i in indices])\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            done_sample = tf.convert_to_tensor(\n",
    "                [float(done_history[i]) for i in indices]\n",
    "            )\n",
    "\n",
    "            # Build the updated Q-values for the sampled future states\n",
    "            # Use the target model for stability\n",
    "\n",
    "            future_rewards = model_target.predict(state_next_sample)\n",
    "            # Q value = reward + discount factor * expected future reward\n",
    "            updated_q_values = rewards_sample + gamma * tf.reduce_max(\n",
    "                future_rewards, axis=1\n",
    "            )\n",
    "\n",
    "            # If final frame set the last value to -1\n",
    "            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
    "\n",
    "            # Create a mask so we only calculate loss on the updated Q-values\n",
    "            masks = tf.one_hot(action_sample, num_actions)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Train the model on the states and updated Q-values\n",
    "                q_values = model(state_sample)\n",
    "\n",
    "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                # Calculate loss between new Q-value and old Q-value\n",
    "                loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "            # Backpropagation\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if move_count % update_target_network == 0:\n",
    "            # update the the target network with new weights\n",
    "            model_target.set_weights(model.get_weights())\n",
    "            # Log details\n",
    "            template = \"running reward: {:.2f} at episode {}, frame count {}\"\n",
    "            print(template.format(running_reward, episode_count, move_count))\n",
    "\n",
    "        # Limit the state and reward history\n",
    "        if len(rewards_history) > max_memory_length:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Update running reward to check condition for solving\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    if len(episode_reward_history) > 100:\n",
    "        del episode_reward_history[:1]\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "\n",
    "    episode_count += 1\n",
    "\n",
    "    print(\"difference: \" + str(env.game.get_observation().points[0] - env.game.get_observation().points[1]) + \" @Episode: \" + str(episode_count) + \" running: reward = \"  + str(running_reward), flush=True)\n",
    "\n",
    "    if running_reward > 400:  # Condition to consider the task solved\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}