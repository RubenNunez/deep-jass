{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Jass Agent with Reinforcement Learning\n",
    "### Fact sheet by Ruben Nunez & Jordan Suter\n",
    "\n",
    "This fact sheet is highly inspired by the paper of Jacop Chapman and Mathias Lechner. <<Deep Q-Learning for Atari Breakout>>\n",
    "\n",
    "We tried to implement the Jass-Agent similarly as described in the paper.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from jass.game.game_util import *\n",
    "from jass.game.game_sim import GameSim\n",
    "from jass.game.game_observation import GameObservation\n",
    "from jass.game.const import *\n",
    "from jass.game.rule_schieber import RuleSchieber\n",
    "from jass.agents.agent import Agent\n",
    "from jass.agents.agent_random_schieber import AgentRandomSchieber\n",
    "from jass.arena.arena import Arena\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "sys.path.append(\"../deep-jass-project\")\n",
    "# noinspection PyUnresolvedReferences\n",
    "from agent_gen1 import AgentGen1\n",
    "# noinspection PyUnresolvedReferences\n",
    "from agent_gen2 import AgentGen2\n",
    "# noinspection PyUnresolvedReferences\n",
    "from agent_gen3 import AgentGen3\n",
    "# noinspection PyUnresolvedReferences\n",
    "from agent_gen4 import AgentGen4\n",
    "# noinspection PyUnresolvedReferences\n",
    "from agent_gen5 import AgentGen5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Setup parameter\n",
    "seed = 42\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "epsilon = 1.0  # Epsilon greedy parameter\n",
    "epsilon_min = 0.1  # Minimum epsilon greedy parameter\n",
    "epsilon_max = 1.0  # Maximum epsilon greedy parameter\n",
    "epsilon_interval = (epsilon_max - epsilon_min)\n",
    "\n",
    "# Rate at which to reduce chance of random action being taken\n",
    "batch_size = 32  # Size of batch taken from replay buffer\n",
    "max_steps_per_episode = 10000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implement the Deep Q-Network\n",
    "This network learns an approximation of the Q-table, which is a mapping between the states and actions that an agent will take.\n",
    "In every state are 36 actions, that can eventually be taken. The environment provides the state, and the action is chosen by selecting the larger of the 36 Q-values predicted in the output layer."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-28 01:00:13.861446: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "num_actions = 36\n",
    "\n",
    "def create_q_model():\n",
    "    # Network defined by the Deepmind paper\n",
    "    inputs = layers.Input(shape=(84, 84, 36,))\n",
    "\n",
    "    # Convolutions on the frames on the screen\n",
    "    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n",
    "    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
    "    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
    "\n",
    "    layer4 = layers.Flatten()(layer3)\n",
    "\n",
    "    layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n",
    "    action = layers.Dense(num_actions, activation=\"linear\")(layer5)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=action)\n",
    "\n",
    "# The first model makes the predictions for Q-values which are used to\n",
    "# make a action.\n",
    "model = create_q_model()\n",
    "# Build a target model for the prediction of future rewards.\n",
    "# The weights of a target model get updated every 10000 steps thus when the\n",
    "# loss between the Q-values is calculated the target Q-value is stable.\n",
    "model_target = create_q_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training process"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "# In the Deepmind paper they use RMSProp however then Adam optimizer\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0) # improves training time\n",
    "\n",
    "# Experience replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "episode_reward_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "trick_count = 0\n",
    "\n",
    "epsilon_random_frames = 50000  # Number of frames to take random action and observe output\n",
    "epsilon_greedy_frames = 1000000.0  # Number of frames for exploration\n",
    "\n",
    "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
    "max_memory_length = 100000 # Maximum replay length\n",
    "update_after_actions = 1  # Train the model after 1 actions\n",
    "update_target_network = 10000  # How often to update the target network\n",
    "loss_function = keras.losses.Huber()  # Using huber loss for stability"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Agent Impl"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# Reinforcement Agent Implementation\n",
    "# by Ruben Nunez & Jordan Suter\n",
    "\n",
    "class AgentReinforcement(Agent):\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_trump_selection_score(cards, trump: int) -> int:\n",
    "        trump_score = [15, 10, 7, 25, 6, 19, 5, 5, 5]\n",
    "        no_trump_score = [9, 7, 5, 2, 1, 0, 0, 0, 0]\n",
    "\n",
    "        score = 0\n",
    "        for card in cards:\n",
    "            suit = int(card / 9)\n",
    "            exact_card = card % 9\n",
    "            score += trump_score[exact_card] if trump == suit else no_trump_score[exact_card]\n",
    "\n",
    "        return score\n",
    "\n",
    "    episode_reward = 0\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._rule = RuleSchieber() # we need a rule object to determine the valid cards\n",
    "\n",
    "    def action_trump(self, obs: GameObservation) -> int:\n",
    "        card_list = convert_one_hot_encoded_cards_to_int_encoded_list(obs.hand)\n",
    "        threshold = 68\n",
    "        scores = [0, 0, 0, 0]\n",
    "        for suit in range(0, 4):\n",
    "            trump_score = self.calculate_trump_selection_score(card_list, suit)\n",
    "            scores[suit] = trump_score\n",
    "\n",
    "        best_score = max(scores)\n",
    "        best_suit = scores.index(best_score)\n",
    "\n",
    "        if best_score <= threshold and obs.player < 1:\n",
    "            return PUSH\n",
    "        else:\n",
    "            return best_suit\n",
    "\n",
    "    def action_play_card(self, obs: GameObservation) -> int:\n",
    "\n",
    "        global epsilon\n",
    "\n",
    "        global action_history\n",
    "        global state_history\n",
    "        global state_next_history\n",
    "        global rewards_history\n",
    "        global done_history\n",
    "        global episode_reward_history\n",
    "        global running_reward\n",
    "        global episode_count\n",
    "        global trick_count\n",
    "\n",
    "        trick_count += 1\n",
    "\n",
    "        # Use epsilon-greedy for exploration\n",
    "        if trick_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            # Take random action\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # Predict action Q-values\n",
    "            # From environment state\n",
    "            state_tensor = tf.convert_to_tensor(state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            action_probs = model(state_tensor, training=False)\n",
    "            # Take best action\n",
    "            action = tf.argmax(action_probs[0]).numpy()\n",
    "        # Decay probability of taking random action\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "        # Apply the sampled action in our environment\n",
    "        state_next, reward, done, _ = None # env.step(action)\n",
    "        state_next = np.array(state_next)\n",
    "\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Save actions and states in replay buffer\n",
    "        action_history.append(action)\n",
    "        state_history.append(state)\n",
    "        state_next_history.append(state_next)\n",
    "        done_history.append(done)\n",
    "        rewards_history.append(reward)\n",
    "        state = state_next\n",
    "\n",
    "        # Get indices of samples for replay buffers\n",
    "        indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
    "\n",
    "        # Using list comprehension to sample from replay buffer\n",
    "        state_sample = np.array([state_history[i] for i in indices])\n",
    "        state_next_sample = np.array([state_next_history[i] for i in indices])\n",
    "        rewards_sample = [rewards_history[i] for i in indices]\n",
    "        action_sample = [action_history[i] for i in indices]\n",
    "        done_sample = tf.convert_to_tensor(\n",
    "            [float(done_history[i]) for i in indices]\n",
    "        )\n",
    "\n",
    "        # Build the updated Q-values for the sampled future states\n",
    "        # Use the target model for stability\n",
    "        future_rewards = model_target.predict(state_next_sample)\n",
    "        # Q value = reward + discount factor * expected future reward\n",
    "        updated_q_values = rewards_sample + gamma * tf.reduce_max(\n",
    "            future_rewards, axis=1\n",
    "        )\n",
    "\n",
    "        # If final frame set the last value to -1\n",
    "        updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
    "\n",
    "        # Create a mask so we only calculate loss on the updated Q-values\n",
    "        masks = tf.one_hot(action_sample, num_actions)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Train the model on the states and updated Q-values\n",
    "            q_values = model(state_sample)\n",
    "\n",
    "            # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "            q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "            # Calculate loss between new Q-value and old Q-value\n",
    "            loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "        # Backpropagation\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # update the the target network with new weights\n",
    "        model_target.set_weights(model.get_weights())\n",
    "        # Log details\n",
    "        template = \"running reward: {:.2f} at episode {}, frame count {}\"\n",
    "        print(template.format(running_reward, episode_count, frame_count))\n",
    "\n",
    "        # Limit the state and reward history\n",
    "        if len(rewards_history) > max_memory_length:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "\n",
    "        # Update running reward to check condition for solving\n",
    "        episode_reward_history.append(episode_reward)\n",
    "        if len(episode_reward_history) > 100:\n",
    "            del episode_reward_history[:1]\n",
    "\n",
    "        running_reward = np.mean(episode_reward_history)\n",
    "\n",
    "        episode_count += 1\n",
    "\n",
    "        if running_reward > 40:  # Condition to consider the task solved\n",
    "            print(\"Solved at episode {}!\".format(episode_count))\n",
    "\n",
    "        valid_cards = self._rule.get_valid_cards_from_obs(obs)\n",
    "        card = np.random.choice(np.flatnonzero(valid_cards))\n",
    "        return card\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Jass loop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/61/3v_jwt596vl__t_nyhy5j3jh0000gp/T/ipykernel_79352/1649675841.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     12\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Team 1 : \"\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marena\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpoints_team_1\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m\": Games won : \"\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marena\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnr_games_played\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mcount\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 14\u001B[0;31m \u001B[0mlocal_arena\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/var/folders/61/3v_jwt596vl__t_nyhy5j3jh0000gp/T/ipykernel_79352/1649675841.py\u001B[0m in \u001B[0;36mlocal_arena\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m     \u001B[0marena\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mArena\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnr_games_to_play\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1000\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[0marena\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_players\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mAgentReinforcement\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mAgentReinforcement\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mAgentReinforcement\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mAgentReinforcement\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m     \u001B[0marena\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplay_all_games\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[0mcount\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/deep-jass-project/lib/python3.9/site-packages/jass/arena/arena.py\u001B[0m in \u001B[0;36mplay_all_games\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    226\u001B[0m         \u001B[0mdealer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mNORTH\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    227\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mgame_id\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_nr_games_to_play\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 228\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplay_game\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdealer\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdealer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    229\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnr_games_played\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_print_every_x_games\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    230\u001B[0m                 \u001B[0mpoints_to_write\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnr_games_played\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_nr_games_to_play\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0;36m40\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/deep-jass-project/lib/python3.9/site-packages/jass/arena/arena.py\u001B[0m in \u001B[0;36mplay_game\u001B[0;34m(self, dealer)\u001B[0m\n\u001B[1;32m    195\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mcards\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m36\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    196\u001B[0m             \u001B[0mobs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_game\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_observation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 197\u001B[0;31m             \u001B[0mcard_action\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_players\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_game\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplayer\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maction_play_card\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    198\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_check_moves_validity\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    199\u001B[0m                 \u001B[0;32massert\u001B[0m \u001B[0mcard_action\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mflatnonzero\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_game\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrule\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_valid_cards_from_obs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/61/3v_jwt596vl__t_nyhy5j3jh0000gp/T/ipykernel_79352/3655857594.py\u001B[0m in \u001B[0;36maction_play_card\u001B[0;34m(self, obs)\u001B[0m\n\u001B[1;32m     73\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     74\u001B[0m         \u001B[0;31m# Apply the sampled action in our environment\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 75\u001B[0;31m         \u001B[0mstate_next\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;31m# env.step(action)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     76\u001B[0m         \u001B[0mstate_next\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate_next\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     77\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "def local_arena():\n",
    "    arena = Arena(nr_games_to_play=1000)\n",
    "    arena.set_players(AgentReinforcement(), AgentReinforcement(), AgentReinforcement(), AgentReinforcement())\n",
    "    arena.play_all_games()\n",
    "\n",
    "    count = 0\n",
    "    for i in range(arena.nr_games_played):\n",
    "        if arena.points_team_0[i] > arena.points_team_1[i]:\n",
    "            count = count + 1\n",
    "\n",
    "    print(\"Team 0 : \" + str(arena.points_team_0.sum()) + \": Games won : \" + str(count))\n",
    "    print(\"Team 1 : \" + str(arena.points_team_1.sum()) + \": Games won : \" + str(arena.nr_games_played - count))\n",
    "\n",
    "local_arena()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}